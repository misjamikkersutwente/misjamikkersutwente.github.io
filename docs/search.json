[
  {
    "objectID": "teaching/payment_models/summary.html",
    "href": "teaching/payment_models/summary.html",
    "title": "Summary",
    "section": "",
    "text": "Summary\nIn summary, this book has no content whatsoever."
  },
  {
    "objectID": "teaching/payment_models/intro.html",
    "href": "teaching/payment_models/intro.html",
    "title": "Introduction",
    "section": "",
    "text": "Introduction\nThis is a book created from markdown and executable code.\nSee @knuth84 for additional discussion of literate programming."
  },
  {
    "objectID": "teaching/payment_models/chapter1.html",
    "href": "teaching/payment_models/chapter1.html",
    "title": "Some nice title",
    "section": "",
    "text": "Some nice title\nAnd here some nice words as well."
  },
  {
    "objectID": "blog/Poweranalysis/index.html",
    "href": "blog/Poweranalysis/index.html",
    "title": "Bayesian Power Analyse",
    "section": "",
    "text": "Insurance company VGZ and some general practitioners from the care group Syntein are starting an experimental form of financing that aligns with working according to the principles of Positive Health. The intervention practices will be funded through a complete subscription-based financing model in the experiment. We expect that these practices will have a lower number of referrals to secondary care compared to the control practices designated for this study. In this document, we explain how we will determine the effect of the intervention through Bayesian analysis, despite having a relatively limited number of intervention practices. We begin by explaining the principles of Bayesian statistics and then show step by step how we construct the data and analysis. We visualize the results and demonstrate how they change with different premises. The installation instructions describe how you can reproduce this document yourself. This blog only describes the analyses; the broader research design will outlined in a separate document.\nNote: This research follows up on a previous experiment where only the general practice in Afferden was funded through a complete subscription fee. In this blog, we sometimes refer to the findings of this experiment. The research results are published here. This blog is also published in Dutch on the NZa github website."
  },
  {
    "objectID": "blog/Poweranalysis/index.html#generating-base-data-based-on-parameters",
    "href": "blog/Poweranalysis/index.html#generating-base-data-based-on-parameters",
    "title": "Bayesian Power Analyse",
    "section": "Generating base data based on parameters",
    "text": "Generating base data based on parameters\nWe use the control practices from the previous study to generate the data for this simulation. These practices had about 0.35 to 0.45 referrals per enrolled patient, with a standard deviation of 0.02. Based on these parameters, we generate a dataset that serves as our “base data.” The control practices receive a random number of referrals between 0.37 and 0.43. The intervention practices receive a fixed number of 0.40 referrals. By maintaining a fixed value for the intervention practices, we can easily relate the effect to the starting value later on."
  },
  {
    "objectID": "blog/Poweranalysis/index.html#adding-an-effect",
    "href": "blog/Poweranalysis/index.html#adding-an-effect",
    "title": "Bayesian Power Analyse",
    "section": "Adding an effect",
    "text": "Adding an effect\nTo this base data, we manually add an effect to simulate the intervention. For example, we add an effect of -0.04 (the number of referrals per patient decreases by 0.04). Relating this to the starting value of the intervention practices (0.4), you see a decrease of 10% (0.04/0.4)."
  },
  {
    "objectID": "blog/Poweranalysis/index.html#bayesian-regression",
    "href": "blog/Poweranalysis/index.html#bayesian-regression",
    "title": "Bayesian Power Analyse",
    "section": "Bayesian regression",
    "text": "Bayesian regression\nAfter introducing the effect, we perform a Bayesian regression on the dataset. Based on the results of the regression, we can assess how accurately and precisely we have estimated the effect, given the inherent noise and variability in the data.\nThe participating parties in the study can choose to achieve a certain minimum effect (and decide, for example, to stop the experiment if this is not achieved). Therefore, we also show how the results can be used to determine the certainty around achieving this minimum effect. The likelihood of achieving the minimum effect (given the inherent noise and variability in the data) is represented in a probability distribution."
  },
  {
    "objectID": "blog/Poweranalysis/index.html#repeated-analysis",
    "href": "blog/Poweranalysis/index.html#repeated-analysis",
    "title": "Bayesian Power Analyse",
    "section": "Repeated Analysis",
    "text": "Repeated Analysis\nWe do not perform the regression just once, but repeat it several times. With each repetition (iteration), we generate a new dataset, while continuing to work with the same parameters. This gives us a series of results that illustrate the stability and reproducibility of our analysis. The repetitions help us understand how effective and reliable our analysis is at estimating effects and assessing probabilities, even in situations where limited data points are available."
  },
  {
    "objectID": "blog/Poweranalysis/index.html#final-note",
    "href": "blog/Poweranalysis/index.html#final-note",
    "title": "Bayesian Power Analyse",
    "section": "Final Note",
    "text": "Final Note\nFinally, we want to note that we have written this simulation for the total number of referrals because we had information on this from previous research. We expect that other interesting outcomes (for example, subcategories of referrals) will contain much more noise than the total number of referrals. However, we can only conduct a proper simulation when we have the referrals from the relevant practices."
  },
  {
    "objectID": "blog/Poweranalysis/index.html#number-of-practices",
    "href": "blog/Poweranalysis/index.html#number-of-practices",
    "title": "Bayesian Power Analyse",
    "section": "number of practices",
    "text": "number of practices\n\nCode# Choose number of intervention practices\n\nIP &lt;- 2\n\n# Choose number of control practices\n\nCP &lt;- 3\n\n\nWe have 2 intervention and 3 control practices. This number can be adjusted in the “number of practices” code block above. For the base data generated below, we opt for an average number of referrals per patient of about 0.4 with a standard deviation of 0.02. These parameters are based on the control practices from the previous study in Afferden. Note, however, that, in the previous study, we only had annual data; these data are generated per quarter. It is not entirely clear whether the variation on a quarterly basis is larger.\nNote: To conduct the analysis, we need:\n\nNumber of referrals per practice (from the healthcare domain)\nNumber of registered patients per quarter"
  },
  {
    "objectID": "blog/Poweranalysis/index.html#trend-over-time",
    "href": "blog/Poweranalysis/index.html#trend-over-time",
    "title": "Bayesian Power Analyse",
    "section": "Trend Over Time",
    "text": "Trend Over Time\n\nCode# Choose a time trend (structural increase/decrease per quarter) that applies to all practices.\n\nTimeTrend &lt;- -0.01\n\n\nIt’s entirely possible that there’s a natural trend (independent of the intervention) in the number of referrals. In the data from the control practices in the Afferden study, it seems that referrals across all practices slightly decrease over time. A time trend might complicate estimating the effect, which is why we’ve also introduced a time trend. This time trend can be adjusted in the code block above."
  },
  {
    "objectID": "blog/Poweranalysis/index.html#effect-size-and-minimum-effect",
    "href": "blog/Poweranalysis/index.html#effect-size-and-minimum-effect",
    "title": "Bayesian Power Analyse",
    "section": "Effect size and minimum Effect",
    "text": "Effect size and minimum Effect\n\nCode# Choose effect size\n\nEffect &lt;- -0.04   # 0.04 represents approximately a 10% decrease in referrals\n\n# Choose minimum effect\n\nMinimumEffect &lt;- 0\n\n# Calculate the percentage reduction in referrals for the text\n\nRE &lt;- (Effect / 0.4) * 100\n\n\nWe manually add an effect to our data. In this analysis, we have added an effect of approximately -10 in referrals. The figure we should obtain from our analysis is -0.04. This effect can be adjusted larger or smaller in the code block above.\nAdditionally, we can determine a minimum effect. In this analysis, the minimum effect we wish to observe is set at 0.\nIn our simulation, we obtain the full probability distribution of effect sizes. Generally (but not necessarily), these probability distributions are reasonably normally distributed. We can then estimate the average effect. This estimate should approximately match the effect we have introduced in our simulation. In this case, the average should be close to -0.04. Since we have a full probability distribution, we can also say something about the likelihood of achieving a certain minimum effect.\nIf we set the minimum effect at 0, we can say based on the probability distribution: There is an x% chance that the number of referrals has decreased."
  },
  {
    "objectID": "blog/Poweranalysis/index.html#aantal-simulaties",
    "href": "blog/Poweranalysis/index.html#aantal-simulaties",
    "title": "Bayesian Power Analyse",
    "section": "Aantal simulaties",
    "text": "Aantal simulaties\n\nCode# Choose the number of regressions to be run\n\n\nIterations &lt;- 20\n\n\nFinally, we can determine the number of regressions by adjusting the number of Iterations in the code block above. Running regressions is time-consuming and memory-intensive, and the plots of the outcomes take up a lot of space. Therefore, it is advised not to set the number of iterations too high. In this analysis, the number of regressions has been set to 20."
  },
  {
    "objectID": "blog/Poweranalysis/index.html#base-data",
    "href": "blog/Poweranalysis/index.html#base-data",
    "title": "Bayesian Power Analyse",
    "section": "Base data",
    "text": "Base data\n\nCodeset.seed(123) # For reproducibility\n\n# Create a tibble with practice identifiers and initial referral rates\ndf1 &lt;- tibble(id = 1:(IP + CP)) %&gt;% \n  # Assign intervention or control status to practices\n  mutate(Intervention = ifelse(test = id &lt;= 2, yes = 1, no = 0)) %&gt;%\n  # Set up average referrals per enrolled patient for the first quarter (the base)\n  # To keep the evaluation straightforward, intervention practices receive the same base percentage\n  mutate(Q1 = c(rep(40, each = IP), sample(x = 37:43, size = 3)) / 100)\n\n\nIn the code snippet above, the base data are generated. Practices are categorized as either intervention or control practices, and for each practice, the average number of referrals for the first quarter is set. We will later simulate 11 additional quarters, with the intervention starting in quarter 9."
  },
  {
    "objectID": "blog/Poweranalysis/index.html#simulation",
    "href": "blog/Poweranalysis/index.html#simulation",
    "title": "Bayesian Power Analyse",
    "section": "Simulation",
    "text": "Simulation\n\nCode# Here we simulate (based on the chosen number of regressions) data repeatedly and then run regressions.\n\n\n\nfullrun &lt;- 0\n\nif(fullrun){\n  \n# create empty vectors for estimates and CIs\n  \nnames &lt;- c(\"9\", \"10\", \"11\", \"12\")\nvar_lst &lt;- sapply(paste0(\"Estimate_\", names), function(x) assign(x, NULL))\nvar_lst2 &lt;-sapply(paste0(\"upper_\", names), function(x) assign(x,NULL))\nvar_lst3 &lt;-sapply(paste0(\"lower_\", names), function(x) assign(x,NULL))\n\nlist2env(var_lst, .GlobalEnv)\nlist2env(var_lst2, .GlobalEnv)\nlist2env(var_lst3, .GlobalEnv)\n\n# create an empty tibble for posterior\n\nsamples &lt;- tibble()\n\n# starting the for-loop\n\nfor(i in 1:Iterations){\n  \n#  # varying set.seeds for reproducibility\n\nset.seed(1000 + i * 10)\n\n # create a tibble with other quarters\n    df &lt;- df1 %&gt;%\n      mutate(Q2 = rnorm(n = 5, mean = (1 + TimeTrend) * Q1, sd = 0.02),\n             Q3 = rnorm(n = 5, mean = (1 + TimeTrend)^2 * Q1, sd = 0.02),\n             Q4 = rnorm(n = 5, mean = (1 + TimeTrend)^3 * Q1, sd = 0.02),\n             Q5 = rnorm(n = 5, mean = (1 + TimeTrend)^4 * Q1, sd = 0.02),\n             Q6 = rnorm(n = 5, mean = (1 + TimeTrend)^5 * Q1, sd = 0.02),\n             Q7 = rnorm(n = 5, mean = (1 + TimeTrend)^6 * Q1, sd = 0.02),\n             Q8 = rnorm(n = 5, mean = (1 + TimeTrend)^7 * Q1, sd = 0.02),\n             # the intervention begins now\n             Q9 = rnorm(n = 5, mean = (1 + TimeTrend)^8 * Q1, sd = 0.02) + Effect * Intervention,\n             Q10 = rnorm(n = 5, mean = (1 + TimeTrend)^9 * Q1, sd = 0.02) + Effect * Intervention,\n             Q11 = rnorm(n = 5, mean = (1 + TimeTrend)^10 * Q1, sd = 0.02) + Effect * Intervention,\n             Q12 = rnorm(n = 5, mean = (1 + TimeTrend)^11 * Q1, sd = 0.02) + Effect * Intervention) %&gt;%\n      # make data long\n      pivot_longer(cols = Q1:Q12, names_to = \"Quarter\", \n                      values_to = \"Referrals\") %&gt;%\n      # make quarter numeric to create the intervention variable\n      mutate(Quarter = str_sub(Quarter, start = 2)) %&gt;%\n      # create our effect variables\n      mutate(DiD_9 = ifelse(test = as.numeric(Quarter) == 9 & Intervention == 1, \n                             yes = 1,\n                             no = 0)) %&gt;%\n      mutate(DiD_10 = ifelse(test = as.numeric(Quarter) == 10 & Intervention == 1, \n                             yes = 1,\n                             no = 0)) %&gt;%\n      mutate(DiD_11 = ifelse(test = as.numeric(Quarter) == 11 & Intervention == 1, \n                             yes = 1,\n                             no = 0)) %&gt;%\n      mutate(DiD_12 = ifelse(test = as.numeric(Quarter) == 12 & Intervention == 1, \n                             yes = 1,\n                             no = 0)) %&gt;%\n      # turn quarter into a factor\n      mutate(Quarter = as.factor(Quarter))\n    \n# Formula\n\nFormula &lt;- \"Referrals ~ 0 + Intercept + id + Quarter + DiD_9 + DiD_10 + DiD_11 + DiD_12\"\n\n# Priors\n\n## non informativce priors\n\nprior1 &lt;- c(set_prior(\"normal(0.4, 0.04)\", class = \"b\", coef = \"Intercept\"),\n            set_prior(\"normal(0, 0.05)\", class = \"b\", coef = \"DiD_9\"),\n            set_prior(\"normal(0, 0.05)\", class = \"b\", coef = \"DiD_10\"),\n            set_prior(\"normal(0, 0.05)\", class = \"b\", coef = \"DiD_11\"),\n            set_prior(\"normal(0, 0.05)\", class = \"b\", coef = \"DiD_12\"))\n\n\nreg1 &lt;- brm(\n  formula = Formula,\n  prior = prior1,\n  warmup = 1000,\n  iter = 2500,\n  data = df,\n  chains = 4, \n  cores = 6,\n  init = \"random\",\n  control = list(adapt_delta = 0.8, max_treedepth = 12),\n  seed = 123,\n  backend = \"cmdstanr\",\n  )\n\n\n\n# Fill Empty Vectors with Summary Numbers (Estimate and CI)\n\n\nEstimate_9[i] = fixef(reg1)[14,1]\nlower_9[i] = fixef(reg1)[14,3]\nupper_9[i] = fixef(reg1)[14,4]\n\nEstimate_10[i] = fixef(reg1)[15,1]\nlower_10[i] = fixef(reg1)[15,3]\nupper_10[i] = fixef(reg1)[15,4]\n\nEstimate_11[i] = fixef(reg1)[16,1]\nlower_11[i] = fixef(reg1)[16,3]\nupper_11[i] = fixef(reg1)[16,4]\n\nEstimate_12[i] = fixef(reg1)[17,1]\nlower_12[i] = fixef(reg1)[17,3]\nupper_12[i] = fixef(reg1)[17,4]\n\n## Create a tibble for each quarter\n\n\nResults_9 &lt;- tibble(Estimate = Estimate_9, lower = lower_9, upper = upper_9) %&gt;%\n            mutate(Quarter = \"Quarter 9\")\n\nResults_10 &lt;- tibble(Estimate = Estimate_10, lower = lower_10, upper = upper_10) %&gt;%\n            mutate(Quarter = \"Quarter 10\")\n\nResults_11 &lt;- tibble(Estimate = Estimate_11, lower = lower_11, upper = upper_11) %&gt;%\n            mutate(Quarter = \"Quarter 11\")\n\nResults_12 &lt;- tibble(Estimate = Estimate_12, lower = lower_12, upper = upper_12) %&gt;%\n            mutate(Quarter = \"Quarter 12\")\n\n## Combine the Tibbles\nResults = rbind(Results_9, Results_10, Results_11, Results_12)\n\n\n# posterior draws\n\nsamples_temp &lt;- tibble(as_draws_df(reg1)) %&gt;%\n  mutate(Run = as.factor(i))\n\nsamples &lt;- samples %&gt;%\n  rbind(samples_temp)\n\n}\n\nsaveRDS(Results, \"Results.RDS\")\nsaveRDS(samples, \"samples.RDS\")\n\n\n} else {\n\nResults &lt;- readRDS(\"Results.RDS\")\nsamples &lt;- readRDS(\"samples.RDS\")\n\n  \n}\n\n\nIn the code above, we repeatedly carry out simulations, each time generating slightly different data. We simulate the average referrals per registered patient for quarters 2 through 12, introducing some random variation and a time trend around the number applicable for the first quarter. This added variation (referred to as noise and set at 0.02) amounts to about 5% of the number of referrals and is based on data from control practices in the Afferden study.\nAfterwards, we perform Bayesian regression on each simulated dataset and save the results. In executing these regressions, we start with an initial and somewhat conservative expectation that the intervention has no effect. However, we allow for uncertainty. Therefore, we set an effect whose probability distribution is normally distributed around 0, with a standard deviation of 0.05. This implies we expect about a 95% chance that the effect falls between -0.1 and 0.1. This translates into an effect of approximately plus or minus 25% on the number of referrals. Very large effects (such as those observed in previous research in Afferden) are considered very unlikely with this initial expectation. For completeness, we have also tested an alternative initial expectation, where we expect a large effect. This has only a limited impact on the results. See the following section on the influence of a different prior."
  },
  {
    "objectID": "blog/DEA/index.html",
    "href": "blog/DEA/index.html",
    "title": "A benchmarking example in R",
    "section": "",
    "text": "In this post we will give you an example of a DEA benchmark performed in R. We will do this with simulated data.\nIn this case study we consider 100 courts. Courts handle ‘disputes’, which result in cases or settlements. Hence, settlements and cases are (perfect) substitutes. On the input side, courts incur costs. We consider several DEA models for benchmarking courts. The aim of this example is to learn how to apply DEA and to demonstrate that DEA is capable to reveal inefficiency. Therefore, we will first simulate our dataset of 100 courts, based on a certain assumption on inefficiency of each court (we will therefore know the’true efficiency’). And then we will use DEA to determine efficiency by means of DEA. By comparing DEA results to the assumed efficiency, we will establish whether DEA is capable to reveal true efficiency."
  },
  {
    "objectID": "blog/DEA/index.html#select-our-inputs-and-outputs",
    "href": "blog/DEA/index.html#select-our-inputs-and-outputs",
    "title": "A benchmarking example in R",
    "section": "Select our inputs and outputs",
    "text": "Select our inputs and outputs\nThe Benchmarking package needs inputs and outputs to be matrices. As long as we want 1 input and 1 ouput, we just need to select the relevant variables.\n\nCodemx &lt;- d1 %&gt;%\n  select(TC)\nmy &lt;- d1 %&gt;%\n  select(Cases)"
  },
  {
    "objectID": "blog/DEA/index.html#running-the-model",
    "href": "blog/DEA/index.html#running-the-model",
    "title": "A benchmarking example in R",
    "section": "Running the model",
    "text": "Running the model\nNow we can run the DEA model:\nWe need to chose the inputs and the outputs (as we have done above), the orientation (since we are interested in input minimization we chose “in”) and returns to scale (CRS)\n\nCodemodel_1 &lt;- dea(mx, my, ORIENTATION = \"in\", RTS= \"crs\")"
  },
  {
    "objectID": "blog/DEA/index.html#looking-at-the-output",
    "href": "blog/DEA/index.html#looking-at-the-output",
    "title": "A benchmarking example in R",
    "section": "Looking at the output",
    "text": "Looking at the output\nThe package Benchmarking produces a list with some information.\nMost interesting information are the scores and the peers.\n\nCodeefficiency_scores &lt;- model_1$eff\n\nd_eff &lt;- as.data.frame(efficiency_scores) %&gt;%\n  mutate(Peers = peers(model_1))\n\nkable(summary(d_eff))\n\n\n\n\nefficiency_scores\nPeers.peer1\n\n\n\n\nMin. :0.4190\nMin. :100\n\n\n\n1st Qu.:0.5835\n1st Qu.:100\n\n\n\nMedian :0.6812\nMedian :100\n\n\n\nMean :0.6896\nMean :100\n\n\n\n3rd Qu.:0.7901\n3rd Qu.:100\n\n\n\nMax. :1.0000\nMax. :100\n\n\n\n\n\nAs we can see, DMU with id 100 is peer for all firms."
  },
  {
    "objectID": "blog/DEA/index.html#evaluation-of-the-model",
    "href": "blog/DEA/index.html#evaluation-of-the-model",
    "title": "A benchmarking example in R",
    "section": "Evaluation of the model",
    "text": "Evaluation of the model\nIn reality we cannot evulate the model, but since we work with simulated data we can see how well our model performs by comparing the “real efficiency scores” with the estimated scores from the model.\nFirst, we will add the efficiency scores to our initial dataset\n\nCoded1 &lt;- d1 %&gt;%\n  mutate(Model_1 = model_1$eff)\n\n\nNow we can plot the modelled inefficiency versus the real inefficiency\n\nCodeggplot(data = d1, aes(x= Efficiency, y=Model_1)) + \n  geom_point(color = \"blue\") + \n  geom_abline(intercept = 0, slope = 1, color = \"red\") + \n  xlim(0.4,1) +\n  ylim(0.4,1) +\n  xlab(\"True efficiency\") +\n  ylab(\"Efficiency scores model 1\") +\n  theme_minimal()\n\n\n\n\n\n\n\nWe can conclude that our model does not perform very good:\n\nmost efficiency scores are underestimated\na few are overstated\n\nThe reason is that we only took 1 output, while in our simulated dataset Courts produced 2 outputs."
  },
  {
    "objectID": "blog/DEA/index.html#select-our-inputs-and-outputs-1",
    "href": "blog/DEA/index.html#select-our-inputs-and-outputs-1",
    "title": "A benchmarking example in R",
    "section": "Select our inputs and outputs",
    "text": "Select our inputs and outputs\nWe will run a model with 1 input (TC) and 2 outputs (Cases and Settlements). First we have to select the right variables as input and outputs. Please note: since we have 2 chosen 2 outputs, we should explicitly transform the dataframe into a matrix.\n\nCodemx_1 &lt;- d1 %&gt;%\n  select(TC)\n\nmy_1a &lt;- d1 %&gt;%\n  select(Cases, Settlements)\n\nmy_1 &lt;- as.matrix(my_1a)"
  },
  {
    "objectID": "blog/DEA/index.html#running-the-model-1",
    "href": "blog/DEA/index.html#running-the-model-1",
    "title": "A benchmarking example in R",
    "section": "Running the model",
    "text": "Running the model\nThe only change compared to the first model is the difference in outputs.\n\nCodemodel_CRS &lt;- dea(mx_1, my_1, ORIENTATION = \"in\", RTS= \"crs\")"
  },
  {
    "objectID": "blog/DEA/index.html#looking-at-the-output-1",
    "href": "blog/DEA/index.html#looking-at-the-output-1",
    "title": "A benchmarking example in R",
    "section": "Looking at the output",
    "text": "Looking at the output\nAgain we can extract the efficiency scores and peers\n\nCodeefficiency_scores_CRS &lt;- model_CRS$eff\n\nPeers &lt;-as.data.frame(peers(model_CRS))\n\nid &lt;- 1: 100\nd_CRS &lt;- as.data.frame(id) %&gt;%\n  mutate(efficiency_scores_CRS = efficiency_scores_CRS)\n\nd_CRS_1 &lt;- cbind(d_CRS, Peers)\n\nkable(summary(d_CRS_1))\n\n\n\n\n\n\n\n\n\n\n\nid\nefficiency_scores_CRS\npeer1\npeer2\n\n\n\n\nMin. : 1.00\nMin. :0.6016\nMin. : 1.00\nMin. : 93.00\n\n\n\n1st Qu.: 25.75\n1st Qu.:0.7206\n1st Qu.: 1.00\n1st Qu.:100.00\n\n\n\nMedian : 50.50\nMedian :0.8093\nMedian : 1.00\nMedian :100.00\n\n\n\nMean : 50.50\nMean :0.8189\nMean : 30.07\nMean : 98.28\n\n\n\n3rd Qu.: 75.25\n3rd Qu.:0.9068\n3rd Qu.: 93.00\n3rd Qu.:100.00\n\n\n\nMax. :100.00\nMax. :1.0000\nMax. :100.00\nMax. :100.00\n\n\n\nNA\nNA\nNA\nNA’s :31\n\n\n\n\n\nWe can that the efficiency scores -in general- have improved. The average score is up from around 69% to nearly 82%.\n\nCodeid &lt;- 1:100\nDiff &lt;- as.data.frame(id) %&gt;%\n  mutate(Difference = round(d_CRS_1$efficiency_scores_CRS - d_eff$efficiency_scores , 2))\n\nggplot(data = Diff, aes(x = reorder(id, Difference), y = Difference)) +\n  geom_bar(stat = \"identity\", color = \"blue\", fill = \"lightblue\") + \n  theme_minimal() +\n  xlab(\"ID\") +\n   theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 5))\n\n\n\n\n\n\n\nWe can also have a closer look at the peers:\n\nCoded_peers_CRS &lt;- d_CRS_1%&gt;%\n  filter(efficiency_scores_CRS == 1)\n\nkable(d_peers_CRS)\n\n\n\nid\nefficiency_scores_CRS\npeer1\npeer2\n\n\n\n1\n1\n1\nNA\n\n\n50\n1\n1\n100\n\n\n93\n1\n93\nNA\n\n\n100\n1\n100\nNA\n\n\n\n\n\nWe now see more peers. In the dataframe DMU’s with id’s 1, 100 and 93 are peer. DMU with ID 50 (has also score 1, but is not a peer)"
  },
  {
    "objectID": "blog/DEA/index.html#evaluation-of-the-model-1",
    "href": "blog/DEA/index.html#evaluation-of-the-model-1",
    "title": "A benchmarking example in R",
    "section": "Evaluation of the model",
    "text": "Evaluation of the model\nIn reality we cannot evulate the model, but since we work with simulated data we can see how well our model performs by comparing the “real efficiency scores” with the estimated scores from the model.\nFirst, we will add the efficiency scores to our initial dataset\n\nCoded1 &lt;- d1 %&gt;%\n  mutate(Model_CRS = model_CRS$eff)\n\n\nNow we can plot the modelled inefficiency versus the real inefficiency\n\nCodeggplot(data = d1, aes(x= Efficiency, y=Model_CRS)) + \n  geom_point(color = \"blue\") + \n  geom_abline(intercept = 0, slope = 1, color = \"red\") + \n  xlim(0.5,1) +\n  ylim(0.5,1) +\n  xlab(\"True efficiency\") +\n  ylab(\"Efficiency scores model 2\") +\n  theme_minimal()\n\n\n\n\n\n\n\nWe can now conclude that the model performs reasonbly well."
  },
  {
    "objectID": "blog/DEA/index.html#running-the-model-2",
    "href": "blog/DEA/index.html#running-the-model-2",
    "title": "A benchmarking example in R",
    "section": "Running the model",
    "text": "Running the model\nWe only have to change the RTS (Returns to scale option) into VRS.\n\nCodemodel_VRS &lt;- dea(mx_1, my_1, ORIENTATION = \"in\", RTS= \"vrs\")\n\n\nAgain we can extract the efficiency scores and peers\n\nCodeefficiency_scores_VRS &lt;- model_VRS$eff\n\nPeers &lt;-as.data.frame(peers(model_VRS))\n\nid &lt;- 1: 100\nd_VRS &lt;- as.data.frame(id) %&gt;%\n  mutate(efficiency_scores_VRS = efficiency_scores_VRS)\n\nd_VRS_1 &lt;- cbind(d_VRS, Peers)\n\nkable(summary(d_VRS_1))\n\n\n\n\n\n\n\n\n\n\n\n\nid\nefficiency_scores_VRS\npeer1\npeer2\npeer3\n\n\n\n\nMin. : 1.00\nMin. :0.6047\nMin. : 1.00\nMin. : 18.00\nMin. : 24.00\n\n\n\n1st Qu.: 25.75\n1st Qu.:0.7433\n1st Qu.: 6.00\n1st Qu.: 20.00\n1st Qu.: 93.00\n\n\n\nMedian : 50.50\nMedian :0.8491\nMedian : 11.00\nMedian : 50.00\nMedian : 93.00\n\n\n\nMean : 50.50\nMean :0.8413\nMean : 22.33\nMean : 57.78\nMean : 92.91\n\n\n\n3rd Qu.: 75.25\n3rd Qu.:0.9523\n3rd Qu.: 26.00\n3rd Qu.: 89.00\n3rd Qu.:100.00\n\n\n\nMax. :100.00\nMax. :1.0000\nMax. :100.00\nMax. :100.00\nMax. :100.00\n\n\n\nNA\nNA\nNA\nNA’s :13\nNA’s :34\n\n\n\n\n\nThe score is slightly higher than under CRS (from approx 82% to 84%)\nWe can also have a closer look at the peers:\n\nCoded_peers_VRS &lt;- d_VRS_1%&gt;%\n  filter(efficiency_scores_VRS == 1)\n\nkable(d_peers_VRS)\n\n\n\nid\nefficiency_scores_VRS\npeer1\npeer2\npeer3\n\n\n\n1\n1\n1\nNA\nNA\n\n\n6\n1\n6\nNA\nNA\n\n\n11\n1\n11\nNA\nNA\n\n\n18\n1\n18\nNA\nNA\n\n\n20\n1\n20\nNA\nNA\n\n\n24\n1\n24\nNA\nNA\n\n\n26\n1\n26\nNA\nNA\n\n\n50\n1\n50\nNA\nNA\n\n\n74\n1\n74\nNA\nNA\n\n\n87\n1\n87\nNA\nNA\n\n\n89\n1\n89\nNA\nNA\n\n\n93\n1\n93\nNA\nNA\n\n\n100\n1\n100\nNA\nNA\n\n\n\n\n\nWe see now much more firms with score 1, being their own peers."
  },
  {
    "objectID": "blog/DEA/index.html#evaluation-of-the-model-2",
    "href": "blog/DEA/index.html#evaluation-of-the-model-2",
    "title": "A benchmarking example in R",
    "section": "Evaluation of the model",
    "text": "Evaluation of the model\nWe can add the efficiency scores to our initial model\n\nCoded1 &lt;- d1 %&gt;%\n  mutate(Model_VRS = model_VRS$eff)\n\n\nNow we can plot the modelled inefficiency versus the real inefficiency\n\nCodeggplot(data = d1, aes(x= Efficiency, y=Model_VRS)) + \n  geom_point(color = \"blue\") + \n  geom_abline(intercept = 0, slope = 1, color = \"red\") + \n  xlim(0.4,1) +\n  ylim(0.4,1) +\n  xlab(\"True efficiency\") +\n  ylab(\"Efficiency scores model 3\") +\n  theme_minimal()\n\n\n\n\n\n\n\nAs we see, in general firms improved (a bit). Especially large or small firms will be efficient be default.\nPlease note:\n\nVRS is always beneficial to the DMU’s\nThe returns to scale choice may depend on:\n\nunderlying technology\npossibility for firms to adjust their scale (by merging or splitting up)"
  },
  {
    "objectID": "blog/Bayesian_Matching/index.html",
    "href": "blog/Bayesian_Matching/index.html",
    "title": "Evaluation of a pediatric Transitional Care Unit in the Netherlands",
    "section": "",
    "text": "The Transitional Care Unit (TCU) consortium has initiated the BRIDGE study to examine the effectiveness of the Jeroen Pit Huis (JPH). The JPH is a unique and innovative TCU situated in close proximity of the Amsterdam UMC. In this TCU, patients and their family reside in separate private home-like apartments as an intermediate step between hospital and home. The families can stay while practicing in, and adapting to, their new reality until they are ready to transition home. In this study, we are measuring differences in outcomes between patients discharges via the TCU JPH and patients discharged directly to home (control group). This includes patients from the Erasmus MC, Radboud umc, UMC Groningen and UMC Utrecht. Patients from the Amsterdam UMC who transition directly from hospital to home without an intermediate stay in the JPH will also be included in the control group.\nIn this blog, we explain how we will determine the effect of the intervention using bayesian inverse probability weighting methods. This blog only describes the analyses; the study protocol is registered at clinical trials.gov (hyperlink maken als nummer bekend is) and broader research design will be published in a scientific journal (if this is published, we will post them here)\nPictures of the Jeroen Pit Huis:\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote: The BRIDGE study is currently ongoing and is expected to continue until at least mid-2025. Certain decisions we have made in this logbook are based on a previous study (H2H study - manuscript in preparation). The H2H study was conducted in preparation for the BRIDGE study due to the lack of existing data on the population of children in the Netherlands who potentially could use a TCU. In this study, the same population as in the BRIDGE study was included, with the objectives of: 1) describing the demographics, clinical characteristics, and course of patients transitioning from hospital to home; and 2) identifying perceived barriers to the postponement of hospital-to-home transitions.If final results are published, we will refer here to a new post including the results. This blog aimed at providing insights into the analyses."
  },
  {
    "objectID": "blog/Bayesian_Matching/index.html#why-bayesian-analysis",
    "href": "blog/Bayesian_Matching/index.html#why-bayesian-analysis",
    "title": "Evaluation of a pediatric Transitional Care Unit in the Netherlands",
    "section": "\n2.1 Why Bayesian analysis?",
    "text": "2.1 Why Bayesian analysis?\n\nNote: The text below is adapted from a previous blog. Read more about the principles of Bayesian analysis in that post.\n\nWe utilize Bayesian analysis principles to evaluate the efficacy of the JPH. This approach is adopted due to the constrained data points available in the BRIDGE study. Unlike traditional significance tests, which focus on determining whether an effect is statistically significant by testing how likely the observed data are conditional on the null hypothesis being true, our Bayesian analysis aims to estimate the probability that the intervention actually has a specific effect given the data. Bayesian methods are particularly beneficial for relatively small sample sizes because they allow the incorporation of prior information and provide more intuitive and interpretable results. Additionally, Bayesian analysis can produce credible intervals that directly reflect the uncertainty about parameter estimates, making it a robust choice for studies with limited data. In Bayesian analysis, we start by explicitly stating our expectation about the effect of the intervention. This initial belief is also known as a “prior.” Although we theoretically expect parental stress and the length of hospital stay to decrease at the intervention practices, we have not yet seen evidence of this. Therefore, we formulate our prior conservatively and assume that the intervention has no effect, with a considerable degree of uncertainty. This means we initially assume that the chance of a positive effect (lower parental stress / fewer hospital days) is as great as a negative effect (higher parental stress / more hospital days).\nNext, we collect data. Based on this data, we adjust our prior beliefs. The adjusted belief is expressed in the so-called ‘posterior belief’. The posterior represents the probability that the intervention has an effect, taking into account both our initial belief (the prior) and the data. In other words: the posterior is a mix of the prior and the data. As we collect more data, the influence of the prior on the value of the posterior decreases (for example, we can get more data by adding intervention and control practices, or when we switch to an analysis at the patient level).\nIn summary, the use of Bayesian analysis allows us to work with limited data and still obtain useful insights about the likelihood of an intervention effect. This is particularly relevant in situations where it is difficult to collect enough data points to perform traditional significance tests. By shifting the focus from mere significance to probability, we can better deal with uncertainty and variability in our results."
  },
  {
    "objectID": "blog/Bayesian_Matching/index.html#quasi-experimental-study-design",
    "href": "blog/Bayesian_Matching/index.html#quasi-experimental-study-design",
    "title": "Evaluation of a pediatric Transitional Care Unit in the Netherlands",
    "section": "\n2.2 Quasi-experimental study design",
    "text": "2.2 Quasi-experimental study design\nThis study is a quasi-experimental prospective cohort study comparing the JPH to other H2H practices. We opted for this design because conducting a Randomized Controlled Trial (RCT) was not feasible. The assignment of patients to the intervention cannot be randomized because children are assigned to a ward/JPH based on the hospital. Geographical location determines a child’s H2H program. Random assignment to specific hospitals is not feasible due to the practical difficulties and negative impact of relocating families from their usual living environment.\nThe question then remains: how to control confounding effects in this study? The problem is that populations between the intervention and control groups differ in characteristics and outcomes (confounding). In an RCT, study subjects are randomly assigned to exposure categories, which helps break any links between exposure and confounders. This random assignment reduces the potential for confounding. If these differences are not controlled for, the intervention effect may be underestimated or overestimated.\n\nExample: Imagine you want to measure the effect of an intervention that helps in getting a job. You want to compare a specific policy program in Rotterdam (intervention) with a existing policy program Amsterdam (control). However, in the intervention group, there are more women who already have a higher chance of getting a job than men. If we do not correct for gender, we may overestimate the intervention effect.\n\nHowever, if an RCT is not feasible (as in our case), there are other options to estimate a causal effect. One of those methods is Difference in Differences (DiD) estimation. The idea behind a DiD is to compare the difference in outcome changes between the treatment group and the control group before and after the intervention. If the effect of the intervention is significant, this should be reflected in a larger difference in outcomes between the treatment and control groups after the implementation of the treatment, compared to before the implementation of the treatment. However, a DiD is also not feasible in our study because we do not have data available before the opening of the TCU. Therefore, we have opted for inverse probability weighting."
  },
  {
    "objectID": "blog/Bayesian_Matching/index.html#inverse-probability-weigthing",
    "href": "blog/Bayesian_Matching/index.html#inverse-probability-weigthing",
    "title": "Evaluation of a pediatric Transitional Care Unit in the Netherlands",
    "section": "\n2.3 Inverse probability weigthing",
    "text": "2.3 Inverse probability weigthing\nTo adjust for confounders we use bayesian inverse probability weighting. The following steps need to be undertaken:\n\nCreate a model that predicts treatment. Use confounders (identified with a Directed Acyclic Graph(DAG)) as the covariates. So: Predicts for each patient the probability that the patient - given their charateristics (confounders) - is in the treatment group. For this, we use Bayesian Logistic regressions (Bernoulli distribution). We refer to this probability as ‘propensity scores’.\nGenerate samples of propensity scores based on the posterior distribution of propensity scores. This will give us a lot of propensity scores.\nConvert those propensity scores into inverse probability of treatment weights (IPTW) using this formula:\n\n\\[\\frac{\\text{Treatment}}{\\text{Propensity score}} + \\frac{1 - \\text{Treatment}}{1 -\\text{Propensity score}}\\]\n\nRun an outcome model using those weights."
  },
  {
    "objectID": "blog/Bayesian_Matching/index.html#directed-acyclic-graph-dag",
    "href": "blog/Bayesian_Matching/index.html#directed-acyclic-graph-dag",
    "title": "Evaluation of a pediatric Transitional Care Unit in the Netherlands",
    "section": "\n3.1 Directed Acyclic Graph (DAG)",
    "text": "3.1 Directed Acyclic Graph (DAG)\nDirected Acyclic Graphs (DAGs) are tools used to represent and analyze causal relationships in statistical models. They consist of nodes representing variables and directed edges indicating the direction of causality.DAGs help identify and control for confounding variables, making them useful for the construction of and communicating about causal models.\nPearl and Mackenzie (2018) provides a popular and accessible introduction to the concepts and applications of DAGs. It illustrates how these graphs can clarify causal questions and guide effective data analysis. See Pearl, Glymour, and Jewell (2016) for a more comprehensive and detailed treatment.\nOur strategy involves mapping out the confounding factors using a directed acyclic graph (DAG) and then controlling for these differences. It is crucial to our approach that our DAG is both accurate and complete.\nFor this simulation we use a simplified generic DAG for two outcomes. Below in the DAG we show the relationship between getting the treatment (going to the JPH) (measured as a 0/1 binary variable where 1 = person went to the JPH) and Length of hospital stay (measured in days) / parental stress (measured on a scale of 0-10, with higher values representing higher amount of stress). Both are confounded by the number of children in the family, if the index patient is de oldest child in the family, is parents are living together, and if they are dutch speaking.\n\nCode# Create DAG\nJP_dag &lt;- dagify(\n  LOS ~ JP + Children + Eldest_child + Living_situation + Dutch,\n  JP ~ Children + Eldest_child + Living_situation + Dutch,\n  Eldest_child ~ Children,\n  exposure = \"JP\",\n  outcome = \"LOS\",\n  coords = list(x = c(LOS = 7, JP = 3, Dutch = 4, Living_situation = 6, Children = 4, Eldest_child = 6),\n                y = c(LOS = 2, JP = 2, Dutch = 1, Living_situation = 1, Children = 3, Eldest_child = 3)),\n  labels = c(LOS = \"Outcome\\n(Length of Stay\\nor Stress)\", JP = \"Jeroen Pit Huis\", Children = \"Number of children\", \n             Eldest_child = \"Patient is eldest child\", Living_situation = \"Living situation\", \n             Dutch = \"Dutch speaking\"))\n\n# Turn DAG into a tidy data frame for plotting\nJP_dag1 &lt;- JP_dag %&gt;% \n  tidy_dagitty() %&gt;%\n  node_status()   # Add column for exposure/outcome/latent\n\nstatus_colors &lt;- c(exposure = \"#01796F\", outcome = \"#F79802\", latent = \"grey50\")\n\n# Fancier graph\nggplot(JP_dag1, aes(x = x, y = y, xend = xend, yend = yend)) +\n  geom_dag_edges() +\n  geom_dag_point(aes(color = status)) +\n  geom_label(aes(label = label, fill = status),\n             color = \"white\", fontface = \"bold\", nudge_y = -0.38, nudge_x = 0.3) +\n  scale_color_manual(values = status_colors, na.value = \"grey20\") +\n  scale_fill_manual(values = status_colors, na.value = \"grey20\") +\n  guides(color = \"none\", fill = \"none\") +\n  theme_dag()\n\n\n\n\n\n\nCodeggsave(\"JP_dag.png\")\n\nSaving 7 x 5 in image\n\n\nWe simulate a data based on our DAG and the parameters of the H2H study mentioned in the introduction. This simulation involves generating data for patients from four different hospitals (AMC, EMC, RUMC, and UMCG) with varying characteristics such as living situations, language proficiency, number of children, and eldest patient status.\nSpecifically, we simulate:\n\nLiving Situation: Proportions of patients living together, alone, or in other arrangements.\nLanguage Proficiency: Likelihood of patients speaking Dutch or other languages based on their hospital.\nNumber of Children: Distribution of patients having different numbers of children.\nLength of Stay (LOS): Estimated hospital stay length influenced by various factors such as hospital type, number of children, language proficiency, and living situation.\nStress Levels: Patient stress levels measured on a Likert scale, accounting for similar influencing factors. To generate the Likert scale for stress levels, we used latent variables, which are underlying factors that influence observed data but are not directly measured. In our simulation, we created a latent variable for stress and added specific influences from factors like eldest patient status, language proficiency, and living situation. We then mapped this latent variable onto a Likert scale (1-10) using predefined cut-off points.\n\n\nCodeset.seed(123)\n\n# Settings: number of patients in 4 different hospitals\nAMC &lt;- 50  # AMC\nEMC &lt;- 60  # EMC\nRUMC &lt;- 40 # RUMC\nUMCG &lt;- 15 # UMCG\n\n# Living situation proportions based on a total number of patients (Total_LS)\nTotal_LS &lt;- 43\nTogether &lt;- 34 / Total_LS\nAlone &lt;- 4 / Total_LS\nOther &lt;- 5 / Total_LS\n\n# Language proportions based on a total number of patients (Total_lang)\nTotal_lang &lt;- 43\nDutch_AMC &lt;- 7 / 10\nDutch_EMC &lt;- 18 / 20\nDutch_RUMC &lt;- 0.95\nDutch_UMCG &lt;- 0.99\n\n# Calculate the total number of patients\nTotal_patients &lt;- sum(AMC, EMC, RUMC, UMCG)\n\n# Effect sizes for length of stay (LOS) and stress\nEffect_LOS &lt;- 7\nEffect_stress &lt;- 0.2\n\n# Cut-off points for latent variable to Likert scores (1-10)\ncut_off &lt;- c(-0.8, -0.6, 0.1, 0.5, 0.7, 0.9, 1, 1.1, 1.2)\n\n# Mean and standard deviation for normally distributed variables\nMU &lt;- 1\nSD &lt;- 0.1\n\n# Create the dataframe (df)\ndf &lt;- tibble(ID = 1:Total_patients) %&gt;%\n  mutate(Hospital = as.factor(ifelse(ID &lt;= AMC, \"AMC\",\n                                     ifelse(ID &lt;= AMC + EMC, \"EMC\",\n                                            ifelse(ID &lt;= AMC + EMC + RUMC, \"RUMC\", \"UMCG\"))))) %&gt;%\n  mutate(Treatment = as.factor(ifelse(Hospital == \"AMC\", 1, 0))) %&gt;%\n  mutate(T_num = as.numeric(as.character(Treatment))) %&gt;%\n  mutate(Living_situation = as.factor(sample(x = c(\"together\", \"alone\", \"other\"), replace = TRUE, \n                                             size = Total_patients, prob = c(Together, Alone, Other)))) %&gt;%\n  mutate(Dutch_speaking = as.factor(ifelse(Hospital == \"AMC\", \n                                           sample(x = c(\"Dutch\", \"Non-Dutch\"), replace = TRUE, \n                                                  size = Total_patients, prob = c(Dutch_AMC, 1 - Dutch_AMC)), \n                                           ifelse(Hospital == \"EMC\", \n                                                  sample(x = c(\"Dutch\", \"Non-Dutch\"), \n                                                         replace = TRUE, size = Total_patients, \n                                                         prob = c(Dutch_EMC, 1 - Dutch_EMC)), \n                                                  ifelse(Hospital == \"RUMC\", \n                                                         sample(x = c(\"Dutch\", \"Non-Dutch\"),\n                                                                replace = TRUE, size = Total_patients, \n                                                                prob = c(Dutch_RUMC, 1 - Dutch_RUMC)), \n                                                         sample(x = c(\"Dutch\", \"Non-Dutch\"), \n                                                                replace = TRUE, \n                                                                size = Total_patients, prob = c(Dutch_UMCG, 1 - Dutch_UMCG))))))) %&gt;%\n  mutate(NL = ifelse(Dutch_speaking == \"Dutch\", 0, 1)) %&gt;%\n  mutate(Number_of_children = ifelse(Hospital == \"AMC\", sample(x = c(1, 2, 3, 4, 5), size = Total_patients, \n                                                               replace = TRUE, prob = c(0.5, 0.3, 0.1, 0.1, 0.1)),\n                                     ifelse(Hospital == \"EMC\", sample(x = c(1, 2, 3, 4, 5), size = Total_patients, \n                                                                      replace = TRUE, prob = c(0.4, 0.35, 0.15, 0.1, 0.1)),\n                                            ifelse(Hospital == \"RUMC\", sample(x = c(1, 2, 3, 4, 5), size = Total_patients, \n                                                                              replace = TRUE, prob = c(0.375, 0.5, 0.1, 0.125, 0.1)),\n                                                   sample(x = c(1, 2, 3, 4, 5), size = Total_patients, \n                                                          replace = TRUE, prob = c(0.2, 0.4, 0.2, 0.1, 0.2)))))) %&gt;%\n  mutate(Eldest_patient = as.factor(ifelse(Number_of_children == 1, 1,\n                                           ifelse(Hospital == \"AMC\", sample(x = c(0, 1), size = Total_patients, replace = TRUE, \n                                                                            prob = c(0.3, 0.7)),\n                                                  sample(x = c(0, 1), size = Total_patients, replace = TRUE,\n                                                         prob = c(0.6, 0.4)))))) %&gt;%\n  mutate(Influence_living_situation = ifelse(Living_situation == \"together\", 0, 5)) %&gt;%\n  mutate(LOS0 = rnorm(n = Total_patients, \n                      mean = 50 + \n                        3 * Number_of_children +\n                        1 * NL +\n                        1 * Influence_living_situation +\n                        1 * as.numeric(Eldest_patient), sd = 2)) %&gt;%\n  mutate(LOS = ifelse(Hospital == \"AMC\", LOS0 - Effect_LOS, LOS0)) %&gt;%\n  mutate(Stress_latent0 = rnorm(n = Total_patients, mean = MU - 0.3 * as.numeric(as.character(Eldest_patient)) - \n                                0.3 * NL - \n                                0.3 * Influence_living_situation, sd = SD)) %&gt;%\n  mutate(Stress_latent = ifelse(test = Hospital == \"AMC\",\n                                yes = Stress_latent0 + Effect_stress,\n                                no = Stress_latent0)) %&gt;%\n  mutate(Stress0 = findInterval(Stress_latent0, cut_off) + 1) %&gt;%\n  mutate(Stress = findInterval(Stress_latent, cut_off) + 1) %&gt;%\n  mutate(Stress_fac = as.factor(Stress)) %&gt;%\n  mutate(Treatment_effect = Stress - Stress0)\n\ndf_CF &lt;- df %&gt;%\n  mutate(Treatment = 0)\n\ndf_hospital &lt;- df %&gt;%\n  select(id = ID, Hospital)"
  },
  {
    "objectID": "blog/Bayesian_Matching/index.html#visualization-of-the-data-outcomes",
    "href": "blog/Bayesian_Matching/index.html#visualization-of-the-data-outcomes",
    "title": "Evaluation of a pediatric Transitional Care Unit in the Netherlands",
    "section": "\n3.2 Visualization of the data (outcomes)",
    "text": "3.2 Visualization of the data (outcomes)\nTo better understand the distribution of our simulated data, we present histograms of the two main outcomes: length of stay (LOS) and stress levels. These visualizations help illustrate the variations and patterns within our synthetic data.\n\n3.2.1 Length of Stay per hospital\n\nCodeggplot(df, aes(x = LOS, fill = Hospital)) +\n  geom_histogram(alpha = 0.5, binwidth = 3) +\n  theme_bw() +\n  labs(title = \"Length of Stay in Days per Hospital\", x = \"\", y = \"\") +\n  facet_wrap(~ Hospital) +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n3.2.2 Stress levels per hospital\n\nCodeggplot(df, aes(x = Stress, fill = Hospital)) +\n  geom_histogram(binwidth = 1, alpha = 0.5) +\n  theme_bw() +\n  labs(title = \"Stress per Hospital\", x = \"\", y = \"\") +\n  facet_wrap(~ Hospital) +\n  scale_x_continuous(breaks = 1:10) +\n  theme(legend.position = \"none\")"
  },
  {
    "objectID": "blog/Bayesian_Matching/index.html#propensity-scores-and-iptws",
    "href": "blog/Bayesian_Matching/index.html#propensity-scores-and-iptws",
    "title": "Evaluation of a pediatric Transitional Care Unit in the Netherlands",
    "section": "\n3.3 Propensity scores and IPTWs",
    "text": "3.3 Propensity scores and IPTWs\nBelow the code how we calculate our propensity scores and IPTWs based on the confounders in our DAG:\n\nCodefullrun &lt;- 0 # saves time\n\nif (fullrun) {\n  \n  model_prop_scores &lt;- brm(\n    formula = Treatment ~ Number_of_children + Eldest_patient + Living_situation + Dutch_speaking,\n    family = bernoulli(),  # Logistic regression\n    data = df,\n    chains = 6, \n    cores = 6, \n    warmup = 500, \n    iter = 1500,\n    seed = 123, \n    backend = \"cmdstanr\"\n  )\n\n  saveRDS(model_prop_scores, \"model_prop_scores.RDS\")\n\n} else {\n  \n  model_prop_scores &lt;- readRDS(\"model_prop_scores.RDS\")\n\n}\n\nWarning: Can't find CmdStan makefile to detect version number. Path may not\npoint to valid installation.\n\n\n\nCode# Extract posterior predicted propensity scores\npred_probs_chains &lt;- posterior_epred(model_prop_scores)\n\n\n\nCodefullrun &lt;- 0  # saves time\n\nif(fullrun){\n\nipw_matrix &lt;- t(pred_probs_chains) %&gt;%\n  as_tibble(.name_repair = \"universal\") %&gt;% \n  # Add treatment column so we can calculate weights\n  mutate(T_num = df$T_num) %&gt;% \n  # Calculate weights\n  mutate(across(starts_with(\"...\"),\n                ~ (T_num / .x) + ((1 - T_num) / (1 - .x))\n  )) %&gt;% \n  # Get rid of treatment column\n  select(-T_num)\n\nsaveRDS(ipw_matrix, \"ipw_matrix.RDS\")\n\n} else {\n  \nipw_matrix &lt;- readRDS(\"ipw_matrix.RDS\")\n\n}\n\nIPTW_help &lt;- ipw_matrix %&gt;% \n  # Convert this matrix to a data frame\n  as_tibble() %&gt;%\n  # Add a column for the draw number\n  mutate(draw = 1:n()) %&gt;% \n  # Make this long so that each draw gets its own row\n  pivot_longer(-draw, names_to = \"row\", values_to = \"weights\") %&gt;% \n  # Clean up the draw number \n  mutate(draw = as.numeric(str_remove(row, \"...\"))) %&gt;%\n  select(-row)"
  },
  {
    "objectID": "blog/Bayesian_Matching/index.html#psuedo-populations",
    "href": "blog/Bayesian_Matching/index.html#psuedo-populations",
    "title": "Evaluation of a pediatric Transitional Care Unit in the Netherlands",
    "section": "\n3.4 Psuedo-populations",
    "text": "3.4 Psuedo-populations\nBefore running the bayesian regressions, it’s helpful to understand what these weight are actually doing behind the scenes.\nWe can learn a few different things from the plot below. Fewer people received the treatment than didn’t—there are more people in the untreated part of the graph.\nThe two groups - intervention and control - aren’t comparable at this point. Therefore we created two pseudo-populations of treated and untreated people.Using the IPTW scores, individual patients are weighted, with patients having a higher likelihood of treatment exposure receiving less weight and patients with a lower likelihood receiving more weight. This results in a weighted dataset in which the confounding effects are reduced.\n\nNote: It’s challenging to visualize how we perform this using the Bayesian method because we have calculated 1500 propensity scores for each patient. To make it visualizable, we have chosen to take the mean of the propensity scores and then calculate the IPTWs based on the mean.\n\n\nCode# We based our graph on the means of the propensity scores and the IPTW\n# Calculate the mean propensity scores and IPTW\nmean_propensity &lt;- colMeans(pred_probs_chains)\ndf_with_weights &lt;- df %&gt;%\n  mutate(propensity = mean_propensity,\n         iptw = (T_num / propensity) + ((1 - T_num) / (1 - propensity)))\n\n# Plot the graph\nggplot() +\n  geom_histogram(data = filter(df_with_weights, T_num == 1), \n                 aes(x = propensity, weight = iptw), bins = 10,\n                 fill = colorspace::lighten(\"#01796F\", 0.35)) + \n  geom_histogram(data = filter(df_with_weights, T_num == 0), \n                 aes(x = propensity, weight = iptw, y = -after_stat(count)), bins = 10,\n                 fill =  colorspace::lighten(\"#F79802\", 0.35)) +\n  geom_histogram(data = filter(df_with_weights, T_num == 1), \n                 aes(x = propensity), bins = 10,\n                 fill = \"#01796F\") + \n  geom_histogram(data = filter(df_with_weights, T_num == 0), \n                 aes(x = propensity, y = -after_stat(count)), bins = 10,\n                 fill =  \"#F79802\") +\n  geom_hline(yintercept = 0) +\n  annotate(geom = \"label\", x = 1, y = 105, label = \"Treated\", \n           fill = \"#01796F\", color = \"white\", hjust = 1) +\n  annotate(geom = \"label\", x = 1, y = -80, label = \"Untreated\", \n           fill = \"#F79802\", color = \"white\", hjust = 1) +\n  annotate(geom = \"label\", x = 1, y = -100, label = \"Untreated pseudo-population\", \n           fill = colorspace::lighten(\"#F79802\", 0.35), color = \"white\", hjust = 1) +\n  annotate(geom = \"label\", x = 1, y = 125, label = \"Treated pseudo-population\", \n           fill = colorspace::lighten(\"#01796F\", 0.35), color = \"white\", hjust = 1) +\n  scale_y_continuous(label = abs) +\n  labs(x = \"Propensity\", y = \"Count\") +\n  coord_cartesian(xlim = c(0.0, 1.05)) +\n  theme_bw()"
  },
  {
    "objectID": "blog/Bayesian_Matching/index.html#length-of-hospital-stay-regression",
    "href": "blog/Bayesian_Matching/index.html#length-of-hospital-stay-regression",
    "title": "Evaluation of a pediatric Transitional Care Unit in the Netherlands",
    "section": "\n4.1 Length of hospital stay regression",
    "text": "4.1 Length of hospital stay regression\nTo evaluate the impact of the JPH intervention on the length of hospital stay (a numeric variable and measured in days), we perform a Gaussian regression analysis.\n\nCodefullrun &lt;- 0  # saves time\n\nif (fullrun) {\n  \n  Number_regressions &lt;- 1500  # moet 1500 zijn\n\n  LDEstimate &lt;- c()\n  LDlower_95CI &lt;- c()\n  LDupper_95CI &lt;- c()\n\n  LDPreds &lt;- tibble(id = 1:165)\n  LDPreds_CF &lt;- tibble(id = 1:165)\n\n  for (i in 1:Number_regressions) {\n    \n     # Print the current iteration\n    message(\"Running iteration: \", i)\n    \n    df_temp &lt;- IPTW_help %&gt;%\n      filter(draw == i)\n\n    df_temp2 &lt;- df %&gt;%\n      bind_cols(df_temp)\n\n    LDFormula &lt;- LOS | weights(weights) ~ Treatment  \n\n    LDreg &lt;- brm(\n      formula = LDFormula,  # our formula\n      data = df_temp2,  # data\n      family = gaussian(),  # model for continuous data\n      warmup = 500, \n      iter = 1500, \n      cores = 6,  # If you have 2 cores on your own computer set this to 2\n      chains = 6,\n      seed = 1729,\n      silent = TRUE,\n      backend = \"cmdstanr\"\n    )\n\n    last_row &lt;- tail(fixef(LDreg), 1)\n\n    LDEstimate[i] &lt;- last_row[1, 1]\n    LDlower_95CI[i] &lt;- last_row[1, 3]\n    LDupper_95CI[i] &lt;- last_row[1, 4]\n\n    LDResults &lt;- tibble(LDEstimate, LDlower_95CI, LDupper_95CI) \n  }\n\n  saveRDS(LDResults, \"LDResults.RDS\")\n\n} else {\n  \n  LDResults &lt;- readRDS(\"LDResults.RDS\")\n\n}\n\n\n\nCodeggplot(data = LDResults, aes(x = LDEstimate)) +\n  geom_density(fill = \"steelblue\", alpha = 0.3) +\n  theme_bw() +\n  geom_vline(xintercept = -1 * Effect_LOS, \n             color = \"firebrick\", \n             linetype = \"dashed\",\n             linewidth = 1) +\n  labs(x = \"Estimate of the treatment coefficient\", y = \"\") +\n  annotate(geom = \"text\", x = Effect_LOS * -1 + 0.7, y = 0.5, \n           label = \"Simulated effect\",\n           color = \"firebrick\") +\n  xlim(-10, 10)\n\n\n\n\n\n\n\nThe plot above shows the distribution of the estimated treatment coefficients for the length of hospital stay (LOS) outcome. The vertical dashed line represents the simulated effect of the JPH intervention on LOS. The distribution of the estimated coefficients indicates the uncertainty around the treatment effect, with the majority of the estimates slightly overstating the simulated effect."
  },
  {
    "objectID": "blog/Bayesian_Matching/index.html#parental-stress-regression",
    "href": "blog/Bayesian_Matching/index.html#parental-stress-regression",
    "title": "Evaluation of a pediatric Transitional Care Unit in the Netherlands",
    "section": "\n4.2 Parental stress regression",
    "text": "4.2 Parental stress regression\nNext, we analyze the impact of the JPH intervention on parental stress levels, which are measured on an ordinal scale. We use a cumulative model to estimate the treatment effect on stress levels. The cumulative model estimates the probability that the stress level exceeds a certain threshold, given the treatment and control conditions.\n\nCodeNumber_regressions &lt;- 1500  # moet 1500 zijn\nfullrun &lt;- 0 # saves time\n\nif(fullrun){\n  \n\n\n\nEstimate &lt;- c()\nlower_95CI &lt;- c()\nupper_95CI &lt;- c()\n\n\n\nfor (i in 1:Number_regressions){\n\ndf_temp &lt;- IPTW_help %&gt;%\n  filter(draw == i)\n\ndf_temp2 &lt;- df %&gt;%\n  bind_cols(df_temp)\n\nFormula &lt;- Stress | weights(weights) ~ Treatment\n\nreg &lt;- brm(\n  formula = Formula,   # onze formule\n  data = df_temp2,            # data\n  family = cumulative(\"probit\"), # model voor ordinal data\n  warmup = 500, \n  iter = 1500, \n  cores = 6, #If you have 2 cores on your own computer set this to 2\n  chains = 6,\n  seed = 1729,\n  silent = TRUE,\n  backend = \"cmdstanr\"\n  \n)\n\nlast_row &lt;- tail(fixef(reg),1)\n\nEstimate[i] &lt;- last_row[1,1]\nlower_95CI[i] = last_row[1,3]\nupper_95CI[i] = last_row[1,4]\n\nResults &lt;- tibble(Estimate, lower_95CI, upper_95CI) \n\n}\n\n\nsaveRDS(Results, \"Results.RDS\")\n\n\n} else {\n  \nResults &lt;- readRDS(\"Results.RDS\")\n\n}\n\nResults &lt;- Results %&gt;%\n  mutate(Iteration = seq_along(Estimate))\n\n\n\nCodeggplot(Results, aes(x = Estimate)) +\n  geom_histogram(aes(y = after_stat(density)), binwidth = 0.05, fill = \"steelblue\", alpha = 0.7) +\n  geom_density(color = \"steelblue4\", linewidth = 1) +\n  labs(x = \"Estimate\", y = \"Density\") +\n  geom_vline(aes(\n    xintercept = round( mean(df$Stress[df$Hospital == \"AMC\"]) - mean(df$Stress[df$Hospital != \"AMC\"]),2)), \n    color = \"firebrick\", \n    linetype = \"dashed\",\n    linewidth = 1) +\n  annotate(geom = \"text\", x = round( mean(df$Stress[df$Hospital == \"AMC\"]) - mean(df$Stress[df$Hospital != \"AMC\"]),2) - 0.14, y =2, \n           label = \"Simulated effect\\nin the data\",\n           color = \"firebrick\") +\n  theme_bw()\n\n\n\n\n\n\n\nThe plot above shows the distribution of the estimated treatment coefficients for the parental stress outcome. The vertical dashed line represents the simulated effect of the JPH intervention on parental stress levels. The distribution of the estimated coefficients indicates the uncertainty around the treatment effect, with the majority of the estimates overstating the effect to some extend."
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Blogs",
    "section": "",
    "text": "Welcome to my blog. Here you will find a collection of posts on various topics."
  },
  {
    "objectID": "blog/index.html#section",
    "href": "blog/index.html#section",
    "title": "Blogs",
    "section": "2025",
    "text": "2025\n\n\n    \n    \n                  \n            April 12, 2025\n        \n        \n            Evaluation of a pediatric Transitional Care Unit in the Netherlands\n            \n            \n                \n                \n                    Innovative care model\n                \n                \n                \n                    Bayesian probability weighting methods\n                \n                \n                \n                    Datasimulation\n                \n                \n            \n            \n            In this post, we demonstrate through data simulation how we will measure the effect of an innovative Transitional Care Unit (TCU) called the Jeroen Pit Huis (JPH). We are using bayesian probability weighting methods.\n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#section-1",
    "href": "blog/index.html#section-1",
    "title": "Blogs",
    "section": "2024",
    "text": "2024\n\n\n    \n    \n                  \n            March 20, 2024\n        \n        \n            Bayesian Power Analyse\n            \n            \n                \n                \n                    Bayesian Statistics\n                \n                \n                \n                    Power Analysis\n                \n                \n                \n                    Difference in Difference\n                \n                \n            \n            \n            This blog provides an example of conducting Bayesian power analyses, focusing on a healthcare study comparing intervention and control practices. It covers setting up simulations, generating data, adjusting for time trends, and estimating effects with Bayesian regression. Installation instructions for necessary software and R packages, alongside tips for running simulations and analyzing results, are also included.\n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "blog/index.html#older-posts",
    "href": "blog/index.html#older-posts",
    "title": "Blogs",
    "section": "Older posts",
    "text": "Older posts\n\n\n    \n    \n                  \n            August 21, 2021\n        \n        \n            A benchmarking example in R\n            \n            \n                \n                \n                    DEA\n                \n                \n            \n            \n            The aim of this example is to learn how to apply DEA and to demonstrate that DEA is capable to reveal inefficiency. Therefore, we will first simulate our dataset of 100 courts, based on a certain assumption on inefficiency of each court (we will therefore know the'true efficiency'). And then we will use DEA to determine efficiency by means of DEA. By comparing DEA results to the assumed efficiency, we will establish whether DEA is capable to reveal true efficiency.\n        \n        \n    \n    \n\n\nNo matching items"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Welcome to my website",
    "section": "",
    "text": "Mastodon\n  \n  \n    \n     Github\n  \n  \n    \n     LinkedIn\n  \n  \n    \n     Email\n  \n\n\n\n\nWelcome to my website\nI am a professor of Health Systems Engineering at the section Health Technology & Services Research (HTSR) of the faculty Behavioural, Management and Social Sciences (BMS) at the University of Twente.\nPlease feel free to contact me if you have any questions or would like to discuss potential projects."
  },
  {
    "objectID": "teaching/index.html",
    "href": "teaching/index.html",
    "title": "Teaching",
    "section": "",
    "text": "Courses"
  },
  {
    "objectID": "teaching/payment_models/index.html",
    "href": "teaching/payment_models/index.html",
    "title": "Preface",
    "section": "",
    "text": "Welcome to the Payment Models in Healthcare e-book.\nThis book explores various payment models used in healthcare, including their mechanisms, advantages, and challenges. The goal is to help readers understand the landscape of healthcare payments and how they impact healthcare delivery and outcomes.\n\n\nThe book is organized into several chapters, each addressing a key aspect of payment models in healthcare:\n\nIntroduction: An overview of healthcare payment models.\nChapter 1: Fee-for-Service: Understanding fee-for-service models.\nSummary: Key takeaways and concluding thoughts.\nReferences: References and further reading.\n\nFeel free to navigate through the chapters using the sidebar or the links above.\n\n\n\nMisja Mikkers is a professor of Health Systems Engineering at the University of Twente, specializing in healthcare systems and payment models.\n\n\n\nThanks to"
  },
  {
    "objectID": "teaching/payment_models/index.html#structure-of-the-book",
    "href": "teaching/payment_models/index.html#structure-of-the-book",
    "title": "Preface",
    "section": "",
    "text": "The book is organized into several chapters, each addressing a key aspect of payment models in healthcare:\n\nIntroduction: An overview of healthcare payment models.\nChapter 1: Fee-for-Service: Understanding fee-for-service models.\nSummary: Key takeaways and concluding thoughts.\nReferences: References and further reading.\n\nFeel free to navigate through the chapters using the sidebar or the links above."
  },
  {
    "objectID": "teaching/payment_models/index.html#about-the-author",
    "href": "teaching/payment_models/index.html#about-the-author",
    "title": "Preface",
    "section": "",
    "text": "Misja Mikkers is a professor of Health Systems Engineering at the University of Twente, specializing in healthcare systems and payment models."
  },
  {
    "objectID": "teaching/payment_models/index.html#acknowledgments",
    "href": "teaching/payment_models/index.html#acknowledgments",
    "title": "Preface",
    "section": "",
    "text": "Thanks to"
  },
  {
    "objectID": "teaching/payment_models/references.html",
    "href": "teaching/payment_models/references.html",
    "title": "References",
    "section": "",
    "text": "References"
  }
]