{
  "hash": "749a730bd05011f6dc4e4d0654a81dc5",
  "result": {
    "markdown": "---\ntitle: \"Bayesian Power Analyse\"\nauthors:\n  - name: Misja Mikkers\n    department: Health Technology & Services Research\n    affiliation: University of Twente\n    location: Enschede, the Netherlands\n    email: m.c.mikkers@utwente.nl\n  - name: Tessa Voesenek\n    department: Long Term Care Regulation\n    affiliation: Dutch Healthcare Authority (NZa)\n    location: Utrecht, the Netherlands\n    email: tvoesenek@nza.nl\ndescription: \"This blog provides an example of conducting Bayesian power analyses, focusing on a healthcare study comparing intervention and control practices. It covers setting up simulations, generating data, adjusting for time trends, and estimating effects with Bayesian regression. Installation instructions for necessary software and R packages, alongside tips for running simulations and analyzing results, are also included.\"\ndate: 03-20-2024\ncategories: \n  - Bayesian Statistics\n  - Power Analysis\n  - Difference in Difference\ndraft: false\n---\n\n\n# Introduction\n\nInsurance company VGZ and some general practitioners from the care group Syntein are starting an experimental form of financing that aligns with working according to the principles of [Positive Health](https://www.iph.nl/en/). The intervention practices will be funded through a complete subscription-based financing model in the experiment. We expect that these practices will have a lower number of referrals to secondary care compared to the control practices designated for this study. In this document, we explain how we will determine the effect of the intervention through Bayesian analysis, despite having a relatively limited number of intervention practices. We begin by explaining the principles of Bayesian statistics and then show step by step how we construct the data and analysis. We visualize the results and demonstrate how they change with different premises. The installation instructions describe how you can reproduce this document yourself. This blog only describes the analyses; the broader research design will outlined in a separate document.\n\nNote: This research follows up on a previous experiment where only the general practice in Afferden was funded through a complete subscription fee. In this blog, we sometimes refer to the findings of this experiment. The research results are [published here](https://research.tilburguniversity.edu/en/publications/beyond-the-clock-exploring-the-causal-relationship-between-genera). This blog is also published in Dutch on the [NZa github website](https://github.com/nzanl/experiment_bekostiging_huisartsen_2023). \n\n# The principles of Bayesian analysis\n\nWe employ the principles of Bayesian analysis to assess the effectiveness of a subscription-based financing model at a number of Syntein general practices. We work with a limited number of data points. Unlike traditional significance tests, which focus on determining whether an effect is statistically significant, our Bayesian analysis aims to estimate the probability that the intervention actually has a specific effect.\n\nIn Bayesian analysis, we start by explicitly stating our expectation about the effect of the experiment. This initial belief is also known as a \"prior.\" Although we theoretically expect the number of referrals to decrease at the intervention practices, we have not yet seen evidence of this. Therefore, we formulate our prior conservatively and assume that the intervention has no effect, with a considerable degree of uncertainty. This means we initially assume that the chance of a positive effect (fewer referrals) is as great as a negative effect (more referrals).\n\nNext, we collect data. Based on this data, we adjust our prior beliefs. The adjusted belief is expressed in the so-called 'posterior'. The posterior represents the probability that the intervention has an effect, taking into account both our initial belief (the prior) and the data. In other words: the posterior is a mix of the prior and the data. As we collect more data, the influence of the prior on the value of the posterior decreases (for example, we can get more data by adding intervention and control practices, or when we switch to an analysis at the patient level).\n\nIn the first part of this simulation, you will see that our conservatively set prior (expecting no effect) results in a posterior that is slightly lower than the actual effect. We also show that with an overly optimistic prior (expecting a larger effect than there actually is), the posterior is slightly higher than the actual effect. However, these differences are minor when we include sufficient uncertainty in our priors. This is evident in the second part of the simulation.\n\nIn summary, the use of Bayesian analysis allows us to work with limited data and still obtain useful insights about the likelihood of an intervention effect. This is particularly relevant in situations where it is difficult to collect enough data points to perform traditional significance tests. By shifting the focus from mere significance to probability, we can better deal with uncertainty and variability in our results.\n\n# How does the simulation work?\n\n## Generating base data based on parameters\n\nWe use the control practices from [the previous study](https://research.tilburguniversity.edu/en/publications/beyond-the-clock-exploring-the-causal-relationship-between-genera) to generate the data for this simulation. These practices had about 0.35 to 0.45 referrals per enrolled patient, with a standard deviation of 0.02. Based on these parameters, we generate a dataset that serves as our \"base data.\" The control practices receive a random number of referrals between 0.37 and 0.43. The intervention practices receive a fixed number of 0.40 referrals. By maintaining a fixed value for the intervention practices, we can easily relate the effect to the starting value later on.\n\n## Adding an effect\n\nTo this base data, we manually add an effect to simulate the intervention. For example, we add an effect of -0.04 (the number of referrals per patient decreases by 0.04). Relating this to the starting value of the intervention practices (0.4), you see a decrease of 10% (0.04/0.4).\n\n## Bayesian regression\n\nAfter introducing the effect, we perform a Bayesian regression on the dataset. Based on the results of the regression, we can assess how accurately and precisely we have estimated the effect, given the inherent noise and variability in the data.\n\nThe participating parties in the study can choose to achieve a certain minimum effect (and decide, for example, to stop the experiment if this is not achieved). Therefore, we also show how the results can be used to determine the certainty around achieving this minimum effect. The likelihood of achieving the minimum effect (given the inherent noise and variability in the data) is represented in a probability distribution.\n\n## Repeated Analysis\n\nWe do not perform the regression just once, but repeat it several times. With each repetition (iteration), we generate a new dataset, while continuing to work with the same parameters. This gives us a series of results that illustrate the stability and reproducibility of our analysis. The repetitions help us understand how effective and reliable our analysis is at estimating effects and assessing probabilities, even in situations where limited data points are available.\n\n## Final Note\n\nFinally, we want to note that we have written this simulation for the total number of referrals because we had information on this from previous research. We expect that other interesting outcomes (for example, subcategories of referrals) will contain much more noise than the total number of referrals. However, we can only conduct a proper simulation when we have the referrals from the relevant practices.\n\n\n# What we can modify in this simulation\n\nIt's possible to run and modify this simulation yourself. This section briefly outlines what's needed to run this simulation on your own. You can adjust all values in the code blocks of this chapter and observe the effects on the outcomes.\n\nFree software is required. Installation instructions can be found at the bottom of this notebook.\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# packages and settings\n\nknitr::opts_chunk$set(eval = TRUE, warning = FALSE, message = FALSE)\n\nscipen = 999\n\n# Installing packages -> only needs to be done once, then they can be loaded\n\n# install.packages(c(\"tidyverse\", \"ggridges\", \"brms\"))\n\n# Loading packages\n\nlibrary(tidyverse) # for data wrangling and plots\nlibrary(ggridges) # for posterior plots\nlibrary(brms) # for Bayesian analysis (interface to Stan)\n```\n:::\n\n\n\n\n\n\n## number of  practices\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Choose number of intervention practices\n\nIP <- 2\n\n# Choose number of control practices\n\nCP <- 3\n```\n:::\n\n\nWe have 2 intervention and 3 control practices. This number can be adjusted in the \"number of practices\" code block above. For the base data generated below, we opt for an average number of referrals per patient of about 0.4 with a standard deviation of 0.02. These parameters are based on the control practices from the previous study in Afferden. Note, however, that, in the previous study, we only had annual data; these data are generated per quarter. It is not entirely clear whether the variation on a quarterly basis is larger.\n\nNote: To conduct the analysis, we need:\n\n- Number of referrals per practice (from the healthcare domain)\n- Number of registered patients per quarter\n\n## Trend Over Time\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Choose a time trend (structural increase/decrease per quarter) that applies to all practices.\n\nTimeTrend <- -0.01\n```\n:::\n\n\nIt's entirely possible that there's a natural trend (independent of the intervention) in the number of referrals. In the data from the control practices in the Afferden study, it seems that referrals across all practices slightly decrease over time. A time trend might complicate estimating the effect, which is why we've also introduced a time trend. This time trend can be adjusted in the code block above.\n\n## Effect size and minimum Effect\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Choose effect size\n\nEffect <- -0.04   # 0.04 represents approximately a 10% decrease in referrals\n\n# Choose minimum effect\n\nMinimumEffect <- 0\n\n# Calculate the percentage reduction in referrals for the text\n\nRE <- (Effect / 0.4) * 100\n```\n:::\n\n\nWe manually add an effect to our data. In this analysis, we have added an effect of approximately -10 in referrals. The figure we should obtain from our analysis is -0.04. This effect can be adjusted larger or smaller in the code block above.\n\nAdditionally, we can determine a minimum effect. In this analysis, the minimum effect we wish to observe is set at 0.\n\nIn our simulation, we obtain the full probability distribution of effect sizes. Generally (but not necessarily), these probability distributions are reasonably normally distributed. We can then estimate the average effect. This estimate should approximately match the effect we have introduced in our simulation. In this case, the average should be close to -0.04. Since we have a full probability distribution, we can also say something about the likelihood of achieving a certain minimum effect.\n\nIf we set the minimum effect at 0, we can say based on the probability distribution: There is an x% chance that the number of referrals has decreased.\n\n## Aantal simulaties\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Choose the number of regressions to be run\n\n\nIterations <- 20\n```\n:::\n\n\nFinally, we can determine the number of regressions by adjusting the number of Iterations in the code block above. Running regressions is time-consuming and memory-intensive, and the plots of the outcomes take up a lot of space. Therefore, it is advised not to set the number of iterations too high. In this analysis, the number of regressions has been set to 20.\n\n# Generating Base Data and Performing Regressions\n\n## Base data\n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123) # For reproducibility\n\n# Create a tibble with practice identifiers and initial referral rates\ndf1 <- tibble(id = 1:(IP + CP)) %>% \n  # Assign intervention or control status to practices\n  mutate(Intervention = ifelse(test = id <= 2, yes = 1, no = 0)) %>%\n  # Set up average referrals per enrolled patient for the first quarter (the base)\n  # To keep the evaluation straightforward, intervention practices receive the same base percentage\n  mutate(Q1 = c(rep(40, each = IP), sample(x = 37:43, size = 3)) / 100)\n```\n:::\n\n\nIn the code snippet above, the base data are generated. Practices are categorized as either intervention or control practices, and for each practice, the average number of referrals for the first quarter is set. We will later simulate 11 additional quarters, with the intervention starting in quarter 9.\n\n\n\n## Simulation\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Here we simulate (based on the chosen number of regressions) data repeatedly and then run regressions.\n\n\n\nfullrun <- 0\n\nif(fullrun){\n  \n# create empty vectors for estimates and CIs\n  \nnames <- c(\"9\", \"10\", \"11\", \"12\")\nvar_lst <- sapply(paste0(\"Estimate_\", names), function(x) assign(x, NULL))\nvar_lst2 <-sapply(paste0(\"upper_\", names), function(x) assign(x,NULL))\nvar_lst3 <-sapply(paste0(\"lower_\", names), function(x) assign(x,NULL))\n\nlist2env(var_lst, .GlobalEnv)\nlist2env(var_lst2, .GlobalEnv)\nlist2env(var_lst3, .GlobalEnv)\n\n# create an empty tibble for posterior\n\nsamples <- tibble()\n\n# starting the for-loop\n\nfor(i in 1:Iterations){\n  \n#  # varying set.seeds for reproducibility\n\nset.seed(1000 + i * 10)\n\n # create a tibble with other quarters\n    df <- df1 %>%\n      mutate(Q2 = rnorm(n = 5, mean = (1 + TimeTrend) * Q1, sd = 0.02),\n             Q3 = rnorm(n = 5, mean = (1 + TimeTrend)^2 * Q1, sd = 0.02),\n             Q4 = rnorm(n = 5, mean = (1 + TimeTrend)^3 * Q1, sd = 0.02),\n             Q5 = rnorm(n = 5, mean = (1 + TimeTrend)^4 * Q1, sd = 0.02),\n             Q6 = rnorm(n = 5, mean = (1 + TimeTrend)^5 * Q1, sd = 0.02),\n             Q7 = rnorm(n = 5, mean = (1 + TimeTrend)^6 * Q1, sd = 0.02),\n             Q8 = rnorm(n = 5, mean = (1 + TimeTrend)^7 * Q1, sd = 0.02),\n             # the intervention begins now\n             Q9 = rnorm(n = 5, mean = (1 + TimeTrend)^8 * Q1, sd = 0.02) + Effect * Intervention,\n             Q10 = rnorm(n = 5, mean = (1 + TimeTrend)^9 * Q1, sd = 0.02) + Effect * Intervention,\n             Q11 = rnorm(n = 5, mean = (1 + TimeTrend)^10 * Q1, sd = 0.02) + Effect * Intervention,\n             Q12 = rnorm(n = 5, mean = (1 + TimeTrend)^11 * Q1, sd = 0.02) + Effect * Intervention) %>%\n      # make data long\n      pivot_longer(cols = Q1:Q12, names_to = \"Quarter\", \n                      values_to = \"Referrals\") %>%\n      # make quarter numeric to create the intervention variable\n      mutate(Quarter = str_sub(Quarter, start = 2)) %>%\n      # create our effect variables\n      mutate(DiD_9 = ifelse(test = as.numeric(Quarter) == 9 & Intervention == 1, \n                             yes = 1,\n                             no = 0)) %>%\n      mutate(DiD_10 = ifelse(test = as.numeric(Quarter) == 10 & Intervention == 1, \n                             yes = 1,\n                             no = 0)) %>%\n      mutate(DiD_11 = ifelse(test = as.numeric(Quarter) == 11 & Intervention == 1, \n                             yes = 1,\n                             no = 0)) %>%\n      mutate(DiD_12 = ifelse(test = as.numeric(Quarter) == 12 & Intervention == 1, \n                             yes = 1,\n                             no = 0)) %>%\n      # turn quarter into a factor\n      mutate(Quarter = as.factor(Quarter))\n    \n# Formula\n\nFormula <- \"Referrals ~ 0 + Intercept + id + Quarter + DiD_9 + DiD_10 + DiD_11 + DiD_12\"\n\n# Priors\n\n## non informativce priors\n\nprior1 <- c(set_prior(\"normal(0.4, 0.04)\", class = \"b\", coef = \"Intercept\"),\n            set_prior(\"normal(0, 0.05)\", class = \"b\", coef = \"DiD_9\"),\n            set_prior(\"normal(0, 0.05)\", class = \"b\", coef = \"DiD_10\"),\n            set_prior(\"normal(0, 0.05)\", class = \"b\", coef = \"DiD_11\"),\n            set_prior(\"normal(0, 0.05)\", class = \"b\", coef = \"DiD_12\"))\n\n\nreg1 <- brm(\n  formula = Formula,\n  prior = prior1,\n  warmup = 1000,\n  iter = 2500,\n  data = df,\n  chains = 4, \n  cores = 6,\n  init = \"random\",\n  control = list(adapt_delta = 0.8, max_treedepth = 12),\n  seed = 123,\n  backend = \"cmdstanr\",\n  )\n\n\n\n# Fill Empty Vectors with Summary Numbers (Estimate and CI)\n\n\nEstimate_9[i] = fixef(reg1)[14,1]\nlower_9[i] = fixef(reg1)[14,3]\nupper_9[i] = fixef(reg1)[14,4]\n\nEstimate_10[i] = fixef(reg1)[15,1]\nlower_10[i] = fixef(reg1)[15,3]\nupper_10[i] = fixef(reg1)[15,4]\n\nEstimate_11[i] = fixef(reg1)[16,1]\nlower_11[i] = fixef(reg1)[16,3]\nupper_11[i] = fixef(reg1)[16,4]\n\nEstimate_12[i] = fixef(reg1)[17,1]\nlower_12[i] = fixef(reg1)[17,3]\nupper_12[i] = fixef(reg1)[17,4]\n\n## Create a tibble for each quarter\n\n\nResults_9 <- tibble(Estimate = Estimate_9, lower = lower_9, upper = upper_9) %>%\n            mutate(Quarter = \"Quarter 9\")\n\nResults_10 <- tibble(Estimate = Estimate_10, lower = lower_10, upper = upper_10) %>%\n            mutate(Quarter = \"Quarter 10\")\n\nResults_11 <- tibble(Estimate = Estimate_11, lower = lower_11, upper = upper_11) %>%\n            mutate(Quarter = \"Quarter 11\")\n\nResults_12 <- tibble(Estimate = Estimate_12, lower = lower_12, upper = upper_12) %>%\n            mutate(Quarter = \"Quarter 12\")\n\n## Combine the Tibbles\nResults = rbind(Results_9, Results_10, Results_11, Results_12)\n\n\n# posterior draws\n\nsamples_temp <- tibble(as_draws_df(reg1)) %>%\n  mutate(Run = as.factor(i))\n\nsamples <- samples %>%\n  rbind(samples_temp)\n\n}\n\nsaveRDS(Results, \"Results.RDS\")\nsaveRDS(samples, \"samples.RDS\")\n\n\n} else {\n\nResults <- readRDS(\"Results.RDS\")\nsamples <- readRDS(\"samples.RDS\")\n\n  \n}\n```\n:::\n\n\nIn the code above, we repeatedly carry out simulations, each time generating slightly different data. We simulate the average referrals per registered patient for quarters 2 through 12, introducing some random variation and a time trend around the number applicable for the first quarter. This added variation (referred to as noise and set at 0.02) amounts to about 5% of the number of referrals and is based on data from control practices in the Afferden study.\n\nAfterwards, we perform Bayesian regression on each simulated dataset and save the results. In executing these regressions, we start with an initial and somewhat conservative expectation that the intervention has no effect. However, we allow for uncertainty. Therefore, we set an effect whose probability distribution is normally distributed around 0, with a standard deviation of 0.05. This implies we expect about a 95% chance that the effect falls between -0.1 and 0.1. This translates into an effect of approximately plus or minus 25% on the number of referrals. Very large effects (such as those observed in previous research in Afferden) are considered very unlikely with this initial expectation. For completeness, we have also tested an alternative initial expectation, where we expect a large effect. This has only a limited impact on the results. See the following section on the influence of a different prior.\n\n# Visualizations of the Estimates\n\nWe graphically display the results of the regressions by quarter. Each graph shows the estimated effect per iteration, represented as a probability distribution. The black vertical line within the distribution represents the estimated mean effect. The green vertical line marks the established effect as we previously manually inputted. We expect the effect estimates to be close to the established effect. Previously, we set whether the effect is positive (fewer referrals) or negative (more referrals). The following interpretation applies when the effect is set to be positive: A black line to the left of the green line indicates that the actual effect is overestimated. When the black line is to the right of the green line, the actual effect is underestimated.\n\nThe vertical red dotted line indicates a zero effect. On the right side of the probability distributions, the chance that the effect is at least equal to, or less than, the desired minimum effect in which we are interested is displayed in blue text. In this case, the minimum effect is set at 0.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nSubplottitle <- paste0(\"The probability distribution of the estimated effect per simulation\\nand in blue the chance that the effect is smaller than \", MinimumEffect)\n\nsamples_MinimumEffect9 <- samples %>%\n  mutate(EffectBelowMinimum = ifelse(test = b_DiD_9 < MinimumEffect,\n                                     yes = 1,\n                                     no = 0)) %>%\n  group_by(Run) %>%\n  summarise(Percentage1 = mean(EffectBelowMinimum)) %>%\n  ungroup() %>%\n  mutate(Percentage = paste0(round(Percentage1 * 100, 0), \" %\"))%>%\n  mutate(Run_extra = as.numeric(Run))\n\nggplot(data = samples, aes(x = b_DiD_9, \n                            y = Run)) +\n  geom_density_ridges2(scale = 0.85, quantile_lines=TRUE,\n                       quantile_fun=function(x,...)mean(x)) +\n  geom_vline(xintercept = 0, color = \"firebrick\", linetype = \"dashed\") +\n  geom_vline(xintercept = Effect, color = \"darkgreen\") +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(y = \" \", x = \"Estimated effect\",\n       title = \"Quarter 9\",\n       subtitle = Subplottitle) +\n  geom_text(data = samples_MinimumEffect9, aes(x = 0.05, y = (as.numeric(Run) + 0.25), label = Percentage),\n            size = 2.5, color = \"steelblue\") +\n  xlim(-0.1, 0.1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plotk9-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples_MinimumEffect10 <- samples %>%\n  mutate(EffectBelowMinimum = ifelse(test = b_DiD_10 < MinimumEffect,\n                                     yes = 1,\n                                     no = 0)) %>%\n  group_by(Run) %>%\n  summarise(Percentage1 = mean(EffectBelowMinimum)) %>%\n  ungroup() %>%\n  mutate(Percentage = paste0(round(Percentage1 * 100, 0), \" %\"))%>%\n  mutate(Run_extra = as.numeric(Run))\n\nggplot(data = samples, aes(x = b_DiD_10, \n                            y = Run)) +\n  geom_density_ridges2(scale = 0.85, quantile_lines=TRUE,\n                       quantile_fun=function(x,...)mean(x)) +\n  geom_vline(xintercept = 0, color = \"firebrick\", linetype = \"dashed\") +\n  geom_vline(xintercept = Effect, color = \"darkgreen\") +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(y = \" \", x = \"Estimated effect\",\n       title = \"Quarter 10\",\n       subtitle = Subplottitle) +\n  geom_text(data = samples_MinimumEffect10, aes(x = 0.05, y = (as.numeric(Run) + 0.25), label = Percentage),\n            size = 2.5, color = \"steelblue\") +\n  xlim(-0.1, 0.1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plotk10-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples_MinimumEffect11 <- samples %>%\n  mutate(EffectBelowMinimum = ifelse(test = b_DiD_11 < MinimumEffect,\n                                     yes = 1,\n                                     no = 0)) %>%\n  group_by(Run) %>%\n  summarise(Percentage1 = mean(EffectBelowMinimum)) %>%\n  ungroup() %>%\n  mutate(Percentage = paste0(round(Percentage1 * 100, 0), \" %\"))%>%\n  mutate(Run_extra = as.numeric(Run))\n\nggplot(data = samples, aes(x = b_DiD_11, \n                            y = Run)) +\n  geom_density_ridges2(scale = 0.85, quantile_lines=TRUE,\n                       quantile_fun=function(x,...)mean(x)) +\n  geom_vline(xintercept = 0, color = \"firebrick\", linetype = \"dashed\") +\n  geom_vline(xintercept = Effect, color = \"darkgreen\") +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(y = \" \", x = \"Estimated effect\",\n       title = \"Quarter 11\",\n       subtitle = Subplottitle) +\n  geom_text(data = samples_MinimumEffect11, aes(x = 0.05, y = (as.numeric(Run) + 0.25), label = Percentage),\n            size = 2.5, color = \"steelblue\") +\n  xlim(-0.1, 0.1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plotk11-1.png){width=672}\n:::\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nsamples_MinimumEffect12 <- samples %>%\n  mutate(EffectBelowMinimum = ifelse(test = b_DiD_12 < MinimumEffect,\n                                     yes = 1,\n                                     no = 0)) %>%\n  group_by(Run) %>%\n  summarise(Percentage1 = mean(EffectBelowMinimum)) %>%\n  ungroup() %>%\n  mutate(Percentage = paste0(round(Percentage1 * 100, 0), \" %\"))%>%\n  mutate(Run_extra = as.numeric(Run))\n\nggplot(data = samples, aes(x = b_DiD_12, \n                            y = Run)) +\n  geom_density_ridges2(scale = 0.85, quantile_lines=TRUE,\n                       quantile_fun=function(x,...)mean(x)) +\n  geom_vline(xintercept = 0, color = \"firebrick\", linetype = \"dashed\") +\n  geom_vline(xintercept = Effect, color = \"darkgreen\") +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(y = \" \", x = \"Estimated effect\",\n       title = \"Quarter 12\",\n       subtitle = Subplottitle) +\n  geom_text(data = samples_MinimumEffect12, aes(x = 0.05, y = (as.numeric(Run) + 0.25), label = Percentage),\n            size = 2.5, color = \"steelblue\") +\n  xlim(-0.1, 0.1)\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/plotk12-1.png){width=672}\n:::\n:::\n\n\n# The Influence of Different Initial Expectations (Priors)\n\nWe ran the simulation again as described above. The only change we made is that we now use a more optimistic initial expectation (prior). Our initial expectation is now that the intervention has an effect of -0.2, which means the intervention leads to a decrease of about 50% in the average number of referrals per registered patient (with an average number of referrals of 0.4). By assigning a significant degree of uncertainty (indicated by a standard deviation of 0.2), this optimistic expectation remains flexible and subject to adjustment based on the collected data.\n\nThe graph below shows the effect of this optimistic prior compared to the conservative prior. We present the averages and standard deviations for both priors. With an actual effect of -0.04 (a 10% decrease), the optimistic prior slightly overestimates the effect, while the conservative prior slightly underestimates it. Choosing the best prior depends on our research goals. Since it's particularly important in this research not to overestimate the effect, we will use a conservative prior for our main analysis.\n\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\n# Here we simulate (based on the chosen number of regressions) data repeatedly and then run regressions.\n\nfullrun <- 0\n\nif(fullrun){\n  \n# create empty vectors for estimates and CIs\n  \nnames <- c(\"9\", \"10\", \"11\", \"12\")\nvar_lst <- sapply(paste0(\"Estimate_\", names), function(x) assign(x, NULL))\nvar_lst2 <-sapply(paste0(\"upper_\", names), function(x) assign(x,NULL))\nvar_lst3 <-sapply(paste0(\"lower_\", names), function(x) assign(x,NULL))\n\nlist2env(var_lst, .GlobalEnv)\nlist2env(var_lst2, .GlobalEnv)\nlist2env(var_lst3, .GlobalEnv)\n\n# create an empty tibble for posterior\n\nsamples <- tibble()\n\n# starting the for-loop\n\nfor(i in 1:Iterations){\n  \n#  # varying set.seeds for reproducibility\n\nset.seed(1000 + i * 10)\n\n # create a tibble with other quarters\n    df <- df1 %>%\n      mutate(Q2 = rnorm(n = 5, mean = (1 + TimeTrend) * Q1, sd = 0.02),\n             Q3 = rnorm(n = 5, mean = (1 + TimeTrend)^2 * Q1, sd = 0.02),\n             Q4 = rnorm(n = 5, mean = (1 + TimeTrend)^3 * Q1, sd = 0.02),\n             Q5 = rnorm(n = 5, mean = (1 + TimeTrend)^4 * Q1, sd = 0.02),\n             Q6 = rnorm(n = 5, mean = (1 + TimeTrend)^5 * Q1, sd = 0.02),\n             Q7 = rnorm(n = 5, mean = (1 + TimeTrend)^6 * Q1, sd = 0.02),\n             Q8 = rnorm(n = 5, mean = (1 + TimeTrend)^7 * Q1, sd = 0.02),\n             # the intervention begins now\n             Q9 = rnorm(n = 5, mean = (1 + TimeTrend)^8 * Q1, sd = 0.02) + Effect * Intervention,\n             Q10 = rnorm(n = 5, mean = (1 + TimeTrend)^9 * Q1, sd = 0.02) + Effect * Intervention,\n             Q11 = rnorm(n = 5, mean = (1 + TimeTrend)^10 * Q1, sd = 0.02) + Effect * Intervention,\n             Q12 = rnorm(n = 5, mean = (1 + TimeTrend)^11 * Q1, sd = 0.02) + Effect * Intervention) %>%\n      # make data long\n      pivot_longer(cols = Q1:Q12, names_to = \"Quarter\", \n                      values_to = \"Referrals\") %>%\n      # make quarter numeric to create the intervention variable\n      mutate(Quarter = str_sub(Quarter, start = 2)) %>%\n      # create our effect variables\n      mutate(DiD_9 = ifelse(test = as.numeric(Quarter) == 9 & Intervention == 1, \n                             yes = 1,\n                             no = 0)) %>%\n      mutate(DiD_10 = ifelse(test = as.numeric(Quarter) == 10 & Intervention == 1, \n                             yes = 1,\n                             no = 0)) %>%\n      mutate(DiD_11 = ifelse(test = as.numeric(Quarter) == 11 & Intervention == 1, \n                             yes = 1,\n                             no = 0)) %>%\n      mutate(DiD_12 = ifelse(test = as.numeric(Quarter) == 12 & Intervention == 1, \n                             yes = 1,\n                             no = 0)) %>%\n      # turn quarter into a factor\n      mutate(Quarter = as.factor(Quarter))\n    \n# Formula\n\nFormula <- \"Referrals ~ 0 + Intercept + id + Quarter + DiD_9 + DiD_10 + DiD_11 + DiD_12\"\n\n\n# Priors\n\n## informative priors\n\nprior2 <- c(set_prior(\"normal(0.4, 0.04)\", class = \"b\", coef = \"Intercept\"),\n            set_prior(\"normal(-0.2, 0.2)\", class = \"b\", coef = \"DiD_9\"),\n            set_prior(\"normal(-0.2, 0.2)\", class = \"b\", coef = \"DiD_10\"),\n            set_prior(\"normal(-0.2, 0.2)\", class = \"b\", coef = \"DiD_11\"),\n            set_prior(\"normal(-0.2, 0.2)\", class = \"b\", coef = \"DiD_12\"))\n\n\nreg2 <- brm(\n  formula = Formula,\n  prior = prior2,\n  warmup = 1000,\n  iter = 2500,\n  data = df,\n  chains = 4, \n  cores = 6,\n  init = \"random\",\n  control = list(adapt_delta = 0.8, max_treedepth = 12),\n  seed = 123,\n  backend = \"cmdstanr\",\n  )\n\n\n\n\n\nEstimate_9[i] = fixef(reg2)[14,1]\nlower_9[i] = fixef(reg2)[14,3]\nupper_9[i] = fixef(reg2)[14,4]\n\nEstimate_10[i] = fixef(reg2)[15,1]\nlower_10[i] = fixef(reg2)[15,3]\nupper_10[i] = fixef(reg2)[15,4]\n\nEstimate_11[i] = fixef(reg2)[16,1]\nlower_11[i] = fixef(reg2)[16,3]\nupper_11[i] = fixef(reg2)[16,4]\n\nEstimate_12[i] = fixef(reg2)[17,1]\nlower_12[i] = fixef(reg2)[17,3]\nupper_12[i] = fixef(reg2)[17,4]\n\n\n\n\nResults_9 <- tibble(Estimate = Estimate_9, lower = lower_9, upper = upper_9) %>%\n            mutate(Quarter = \"Quarter 9\")\n\nResults_10 <- tibble(Estimate = Estimate_10, lower = lower_10, upper = upper_10)%>%\n            mutate(Quarter = \"Quarter 10\")\n\nResults_11 <- tibble(Estimate = Estimate_11, lower = lower_11, upper = upper_11)%>%\n            mutate(Quarter = \"Quarter 11\")\n\nResults_12 <- tibble(Estimate = Estimate_12, lower = lower_12, upper = upper_12) %>%\n            mutate(Quarter = \"Quarter 12\")\n\n\n\nResults = rbind(Results_9, Results_10, Results_11, Results_12)\n\n\n# posterior draws\n\nsamples_temp <- tibble(as_draws_df(reg2)) %>%\n  mutate(Run = as.factor(i))\n\nsamples <- samples %>%\n  rbind(samples_temp)\n\n}\n\nsaveRDS(Results, \"Results2.RDS\")\nsaveRDS(samples, \"samples2.RDS\")\n\n\n} else {\n\nResults2 <- readRDS(\"Results2.RDS\")\nsamples2 <- readRDS(\"samples2.RDS\")\n\n  \n}\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\nRR <- Results %>%\n  group_by(Quarter) %>%\n  summarise(ME = mean(Estimate), ML = mean(lower), MU = mean(upper)) %>%\n  ungroup() %>%\n  mutate(Prior = \"conservative\")\n\n\nRR2 <-Results2 %>% \n  group_by(Quarter) %>%\n  summarise(ME = mean(Estimate), ML = mean(lower), MU = mean(upper)) %>%\n  ungroup() %>%\n  mutate(Prior = \"excessively\\noptimistic\")\n\nRR3 <- rbind(RR, RR2) \n\nggplot(data = RR3, aes(x = ML, xend = MU, y = Prior, yend = Prior)) +\n  geom_segment() +\n  geom_point(aes(x = ME, y = Prior), size = 2) +\n  facet_wrap(~factor(Quarter, levels=c(\"Quarter 9\",\n                                        \"Quarter 10\",\n                                        \"Quarter 11\",\n                                        \"Quarter 12\"))) +\n  geom_vline(xintercept = 0, color = \"firebrick\", linetype = \"dashed\") +\n  geom_vline(xintercept = Effect, color = \"darkgreen\") +\n  xlim(-0.2, 0.2) +\n  theme_bw() +\n  theme(legend.position = \"none\") +\n  labs(y = \"\", x = \"Estimated effect\")\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/comparison of priors-1.png){width=672}\n:::\n:::\n\n\n# Installation Instructions\n\nThis notebook is required to proceed. Additionally, free software needs to be installed:\n\n- RStudio and R [https://RStudio.com/download/rstudio-desktop/](https://posit.co/download/rstudio-desktop/ \"RStudio and R\")\n- Stan [https://learnb4ss.github.io/learnB4SS/articles/install-brms.html](https://learnb4ss.github.io/learnB4SS/articles/install-brms.html) (Stan is used for Bayesian estimation)\n\nYou only need to open RStudio. If you are installing RStudio for the first time, it will likely prompt you to install a number of packages (collections of functions). It is necessary to do so. RStudio installs these packages itself.\n\nIn addition, a number of packages (see the `library()` code in the settings code block at the top) need to be installed. This only needs to be done once. To do this, you should remove the hash (`#`) before `install.packages()` and run the code with the play button. Afterward, you can put the hash back (then the code is not run).\n\nWith the \"Render\" button (next to the blue arrow, about in the middle of the toolbar), you can regenerate this HTML. Note: The estimations take a lot of time. Therefore, a function named `fullrun()` has been created. The recommendation is to first run the 2 simulation code blocks after a change with `fullrun <- 1` and then set `fullrun` back to `fullrun <- 0` and run each simulation separately. After doing that, you can use the Render button to create a new HTML.\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}