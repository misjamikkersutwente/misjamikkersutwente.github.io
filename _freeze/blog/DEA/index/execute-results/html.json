{
  "hash": "5afd55f906ff76bd92c35d44d51b21d6",
  "result": {
    "markdown": "---\ntitle: \"A benchmarking example in R\"\nauthors:\n  - name: Misja Mikkers\n    department: Health Technology & Services Research\n    affiliation: University of Twente\n    location: Enschede, the Netherlands\n    email: m.c.mikkers@utwente.nl\n  - name: Victoria Shestalova\n    department: Economic and Medical Bureau\n    affiliation: Dutch Healthcare Authority (NZa)\n    location: Utrecht, the Netherlands\n    email: vshestalova@nza.nl\ndescription: \"The aim of this example is to learn how to apply DEA and to demonstrate that DEA is capable to reveal inefficiency. Therefore, we will first simulate our dataset of 100 courts, based on a certain assumption on inefficiency of each court (we will therefore know the'true efficiency'). And then we will use DEA to determine efficiency by means of DEA. By comparing DEA results to the assumed efficiency, we will establish whether DEA is capable to reveal true efficiency.\"\ndate: 08-21-2021\ncategories: \n  - DEA\ndraft: false\n---\n\n\n\n# Introduction\n\nIn this post we will give you an example of a DEA benchmark performed in R. We will do this with simulated data.\n\nIn this case study we consider 100 courts. Courts handle 'disputes', which result in cases or settlements. Hence, settlements and cases are (perfect) substitutes. On the input side, courts incur costs. We consider several DEA models for benchmarking courts. \nThe aim of this example is to learn how to apply DEA and to demonstrate that DEA is capable to reveal inefficiency. Therefore, we will first simulate our dataset of 100 courts, based on a certain assumption on inefficiency of each court (we will therefore know the'true efficiency'). And then we will use DEA to determine efficiency by means of DEA. By comparing DEA results to the assumed efficiency, we will establish whether DEA is capable to reveal true efficiency.\n\n# Packages\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary(tidyverse) # for manipulating data and plots\nlibrary(Benchmarking) # for DEA\nlibrary(PerformanceAnalytics) # for correlation plots\nlibrary(knitr) # for tables\n```\n:::\n\n\n# Simulation of data\n\nWe will first simulate some simple data of 100 Courts. The courts produce Solutions to disputes (\"production\"), which are \"Cases\" and \"Settlements\"  \n\nID: just an identification number that we will not use:\nTotal_Eff_Cost : Is the \"real efficient total cost\", not observed in practice\nEfficiency: This is the \"real efficiency score\", not observed in practice. Because we want to have some efficient firms for this examples we make sure that firms with id's 1, 50 and 100 become efficient.\nTotal cost: The total cost we observe and can use in our benchmark as input\nCases: The number of verdicts written by each court\nSettlements: Cases that did not come to a verdict (e.g. plea guilty or compromise)\nProduction: The total of cases and settlements\n\nThe data is simulated so that:\n\n- all inefficiency leads to higher cost\n- production is a (lineair) function of total efficient cost\n- which means we have chosen for a CRS technology (see discussion later)\n- settlements and cases are subsitutes\n \n\n\n::: {.cell}\n\n```{.r .cell-code}\nset.seed(123)\nid <- 1:100 \nd1 <- as.data.frame(id) %>%\n  mutate(Total_Eff_Cost = round(runif(100, 100, 500),0)) %>%\n  mutate(Efficiency_1 = round(runif(100, 0.6, 1 ),2))  %>%\n  mutate(Efficiency = ifelse(id == 1 | id ==50 | id ==100, 1, Efficiency_1)) %>%\n  mutate(TC = round(Total_Eff_Cost * (1/Efficiency), 0)) %>%\n  mutate(Production = round(15 * Total_Eff_Cost,0)) %>%\n  mutate(Share_cases = round(runif(100, 0.4, 0.7),2)) %>%\n  mutate(Cases = round(Share_cases * Production, 0)) %>%\n  mutate(Settlements = round((1-Share_cases) * Production, 0))\n```\n:::\n\n\n\nWe can inspect the observable data:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_CC <- d1 %>%\n  select(TC, Cases, Settlements)\n\nsuppressWarnings(chart.Correlation(d_CC))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-3-1.png){width=672}\n:::\n:::\n\n\n# Model 1 with 1 input (TC) and 1 output (Cases)\n\nFor this model we chose to run a CRS model with 1 input: TC (Total Cost) and 1 output (Cases).\n\nSince Settlements and Cases are subsitutes, firms with relatively more Settlements should score worse.\n\n## Select our inputs and outputs\n\nThe `Benchmarking` package needs inputs and outputs to be matrices. As long as we want 1 input and 1 ouput, we just need to select the relevant variables.\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmx <- d1 %>%\n  select(TC)\nmy <- d1 %>%\n  select(Cases)\n```\n:::\n\n\n## Running the model\n\nNow we can run the DEA model:\n\nWe need to chose the inputs and the outputs (as we have done above), the orientation (since we are interested in input minimization we chose \"in\") and returns to scale (CRS) \n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_1 <- dea(mx, my, ORIENTATION = \"in\", RTS= \"crs\")\n```\n:::\n\n\n\n## Looking at the output\n\nThe package `Benchmarking` produces a list with some information.\n\nMost interesting information are the scores and the peers.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nefficiency_scores <- model_1$eff\n\nd_eff <- as.data.frame(efficiency_scores) %>%\n  mutate(Peers = peers(model_1))\n\nkable(summary(d_eff))\n```\n\n::: {.cell-output-display}\n|   |efficiency_scores | Peers.peer1  |\n|:--|:-----------------|:-------------|\n|   |Min.   :0.4190    |Min.   :100   |\n|   |1st Qu.:0.5835    |1st Qu.:100   |\n|   |Median :0.6812    |Median :100   |\n|   |Mean   :0.6896    |Mean   :100   |\n|   |3rd Qu.:0.7901    |3rd Qu.:100   |\n|   |Max.   :1.0000    |Max.   :100   |\n:::\n:::\n\n\nAs we can see, DMU with id 100 is peer for all firms.\n\n## Evaluation of the model\n\nIn reality we cannot evulate the model, but since we work with simulated data we can see how well our model performs by comparing the \"real efficiency scores\" with the estimated scores from the model.\n\nFirst, we will add the efficiency scores to our initial dataset\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd1 <- d1 %>%\n  mutate(Model_1 = model_1$eff)\n```\n:::\n\n\nNow we can plot the modelled inefficiency versus the real inefficiency\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = d1, aes(x= Efficiency, y=Model_1)) + \n  geom_point(color = \"blue\") + \n  geom_abline(intercept = 0, slope = 1, color = \"red\") + \n  xlim(0.4,1) +\n  ylim(0.4,1) +\n  xlab(\"True efficiency\") +\n  ylab(\"Efficiency scores model 1\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-8-1.png){width=672}\n:::\n:::\n\n\n\nWe can conclude that our model does not perform very good:\n\n1. most efficiency scores are underestimated\n2. a few are overstated\n\n\nThe reason is that we only took 1 output, while in our simulated dataset Courts produced 2 outputs.\n\n# Full model (1 input, 2 outputs) CRS\n\n## Select our inputs and outputs\n\nWe will run a model with 1 input (TC) and 2 outputs (Cases and Settlements). First we have to select the right variables as input and outputs. Please note: since we have 2 chosen 2 outputs, we should explicitly transform the dataframe into a matrix.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmx_1 <- d1 %>%\n  select(TC)\n\nmy_1a <- d1 %>%\n  select(Cases, Settlements)\n\nmy_1 <- as.matrix(my_1a)\n```\n:::\n\n\n\n\n## Running the model\n\n\nThe only change compared to the first model is the difference in outputs.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_CRS <- dea(mx_1, my_1, ORIENTATION = \"in\", RTS= \"crs\")\n```\n:::\n\n\n## Looking at the output\n\nAgain we can extract the efficiency scores and peers\n\n\n::: {.cell}\n\n```{.r .cell-code}\nefficiency_scores_CRS <- model_CRS$eff\n\nPeers <-as.data.frame(peers(model_CRS))\n\nid <- 1: 100\nd_CRS <- as.data.frame(id) %>%\n  mutate(efficiency_scores_CRS = efficiency_scores_CRS)\n\nd_CRS_1 <- cbind(d_CRS, Peers)\n\nkable(summary(d_CRS_1))\n```\n\n::: {.cell-output-display}\n|   |      id       |efficiency_scores_CRS |    peer1      |    peer2      |\n|:--|:--------------|:---------------------|:--------------|:--------------|\n|   |Min.   :  1.00 |Min.   :0.6016        |Min.   :  1.00 |Min.   : 93.00 |\n|   |1st Qu.: 25.75 |1st Qu.:0.7206        |1st Qu.:  1.00 |1st Qu.:100.00 |\n|   |Median : 50.50 |Median :0.8093        |Median :  1.00 |Median :100.00 |\n|   |Mean   : 50.50 |Mean   :0.8189        |Mean   : 30.07 |Mean   : 98.28 |\n|   |3rd Qu.: 75.25 |3rd Qu.:0.9068        |3rd Qu.: 93.00 |3rd Qu.:100.00 |\n|   |Max.   :100.00 |Max.   :1.0000        |Max.   :100.00 |Max.   :100.00 |\n|   |NA             |NA                    |NA             |NA's   :31     |\n:::\n:::\n\n\nWe can that the efficiency scores -in general- have improved. The average score is up from around 69% to nearly 82%. \n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nid <- 1:100\nDiff <- as.data.frame(id) %>%\n  mutate(Difference = round(d_CRS_1$efficiency_scores_CRS - d_eff$efficiency_scores , 2))\n\nggplot(data = Diff, aes(x = reorder(id, Difference), y = Difference)) +\n  geom_bar(stat = \"identity\", color = \"blue\", fill = \"lightblue\") + \n  theme_minimal() +\n  xlab(\"ID\") +\n   theme(axis.text.x = element_text(angle = 90, hjust = 1, size = 5))\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-12-1.png){width=672}\n:::\n:::\n\n\n\nWe can also have a closer look at the peers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_peers_CRS <- d_CRS_1%>%\n  filter(efficiency_scores_CRS == 1)\n\nkable(d_peers_CRS)\n```\n\n::: {.cell-output-display}\n|  id| efficiency_scores_CRS| peer1| peer2|\n|---:|---------------------:|-----:|-----:|\n|   1|                     1|     1|    NA|\n|  50|                     1|     1|   100|\n|  93|                     1|    93|    NA|\n| 100|                     1|   100|    NA|\n:::\n:::\n\n\nWe now see more peers. In the dataframe DMU's with id's 1, 100 and 93 are peer. DMU with ID 50 (has also score 1, but is not a peer)\n\n## Evaluation of the model\n\nIn reality we cannot evulate the model, but since we work with simulated data we can see how well our model performs by comparing the \"real efficiency scores\" with the estimated scores from the model.\n\nFirst, we will add the efficiency scores to our initial dataset\n\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd1 <- d1 %>%\n  mutate(Model_CRS = model_CRS$eff)\n```\n:::\n\n\n\n\nNow we can plot the modelled inefficiency versus the real inefficiency\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = d1, aes(x= Efficiency, y=Model_CRS)) + \n  geom_point(color = \"blue\") + \n  geom_abline(intercept = 0, slope = 1, color = \"red\") + \n  xlim(0.5,1) +\n  ylim(0.5,1) +\n  xlab(\"True efficiency\") +\n  ylab(\"Efficiency scores model 2\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-15-1.png){width=672}\n:::\n:::\n\n\nWe can now conclude that the model performs reasonbly well.\n\n\n\n# Model 2 VRS\n\nWe will now estimate the same model with VRS.\n\n## Running the model\n\nWe only have to change the RTS (Returns to scale option) into VRS.\n\n\n::: {.cell}\n\n```{.r .cell-code}\nmodel_VRS <- dea(mx_1, my_1, ORIENTATION = \"in\", RTS= \"vrs\")\n```\n:::\n\n\nAgain we can extract the efficiency scores and peers\n\n\n::: {.cell}\n\n```{.r .cell-code}\nefficiency_scores_VRS <- model_VRS$eff\n\nPeers <-as.data.frame(peers(model_VRS))\n\nid <- 1: 100\nd_VRS <- as.data.frame(id) %>%\n  mutate(efficiency_scores_VRS = efficiency_scores_VRS)\n\nd_VRS_1 <- cbind(d_VRS, Peers)\n\nkable(summary(d_VRS_1))\n```\n\n::: {.cell-output-display}\n|   |      id       |efficiency_scores_VRS |    peer1      |    peer2      |    peer3      |\n|:--|:--------------|:---------------------|:--------------|:--------------|:--------------|\n|   |Min.   :  1.00 |Min.   :0.6047        |Min.   :  1.00 |Min.   : 18.00 |Min.   : 24.00 |\n|   |1st Qu.: 25.75 |1st Qu.:0.7433        |1st Qu.:  6.00 |1st Qu.: 20.00 |1st Qu.: 93.00 |\n|   |Median : 50.50 |Median :0.8491        |Median : 11.00 |Median : 50.00 |Median : 93.00 |\n|   |Mean   : 50.50 |Mean   :0.8413        |Mean   : 22.33 |Mean   : 57.78 |Mean   : 92.91 |\n|   |3rd Qu.: 75.25 |3rd Qu.:0.9523        |3rd Qu.: 26.00 |3rd Qu.: 89.00 |3rd Qu.:100.00 |\n|   |Max.   :100.00 |Max.   :1.0000        |Max.   :100.00 |Max.   :100.00 |Max.   :100.00 |\n|   |NA             |NA                    |NA             |NA's   :13     |NA's   :34     |\n:::\n:::\n\n\nThe score is slightly higher than under CRS (from approx 82% to 84%)\n\nWe can also have a closer look at the peers:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd_peers_VRS <- d_VRS_1%>%\n  filter(efficiency_scores_VRS == 1)\n\nkable(d_peers_VRS)\n```\n\n::: {.cell-output-display}\n|  id| efficiency_scores_VRS| peer1| peer2| peer3|\n|---:|---------------------:|-----:|-----:|-----:|\n|   1|                     1|     1|    NA|    NA|\n|   6|                     1|     6|    NA|    NA|\n|  11|                     1|    11|    NA|    NA|\n|  18|                     1|    18|    NA|    NA|\n|  20|                     1|    20|    NA|    NA|\n|  24|                     1|    24|    NA|    NA|\n|  26|                     1|    26|    NA|    NA|\n|  50|                     1|    50|    NA|    NA|\n|  74|                     1|    74|    NA|    NA|\n|  87|                     1|    87|    NA|    NA|\n|  89|                     1|    89|    NA|    NA|\n|  93|                     1|    93|    NA|    NA|\n| 100|                     1|   100|    NA|    NA|\n:::\n:::\n\n\nWe see now much more firms with score 1, being their own peers.\n\n## Evaluation of the model\n\nWe can add the efficiency scores to our initial model\n\n\n::: {.cell}\n\n```{.r .cell-code}\nd1 <- d1 %>%\n  mutate(Model_VRS = model_VRS$eff)\n```\n:::\n\n\n\n\nNow we can plot the modelled inefficiency versus the real inefficiency\n\n\n::: {.cell}\n\n```{.r .cell-code}\nggplot(data = d1, aes(x= Efficiency, y=Model_VRS)) + \n  geom_point(color = \"blue\") + \n  geom_abline(intercept = 0, slope = 1, color = \"red\") + \n  xlim(0.4,1) +\n  ylim(0.4,1) +\n  xlab(\"True efficiency\") +\n  ylab(\"Efficiency scores model 3\") +\n  theme_minimal()\n```\n\n::: {.cell-output-display}\n![](index_files/figure-html/unnamed-chunk-20-1.png){width=672}\n:::\n:::\n\n\nAs we see, in general firms improved (a bit). Especially large or small firms will be efficient be default.\n\nPlease note:\n\n- VRS is always beneficial to the DMU's\n- The returns to scale choice may depend on:\n    - underlying technology\n    - possibility for firms to adjust their scale (by merging or splitting up)\n\n\n\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}